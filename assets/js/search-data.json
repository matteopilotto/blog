{
  
    
        "post0": {
            "title": "How to fine-tune a transformer model with Hugging Face ü§ó",
            "content": ". Part 1 - Data pre-processing and fine-tuning . &#128278; . In A quick review of natural language processing we will introduce transformer-based models and how they can help us to deal with natural language tasks. | In Loading the data we will learn how to load a dataset from the Hugging Face Hub. | In Taking a bird‚Äôs-eye view of the ML pipeline for NLP we will learn about a common pipeline to fine tune language models. | In Pre-processing phase we will learn how to prepare a dataset for training a language model. | In Training phase we will discover the necessary steps to finetune a transformer model to perform a down-stream task. | In Post-processing/Evaluation phase we will see how to evaluate a transformer-based model. | In Improving the pipeline we will explore different ways to enhance the training loop. | Additional resources contains extra materials to delve into transformers. | . &#10024; A quick review of natural language processing (NLP) &#10024; . Natural language processing (NLP) is the branch of AI that studies how computers can process human language in the form of text or voice and make sense of those words. . NLP techniques can be used to solve a large variety of tasks such as:- sentiment analysis- machine translation . natural language generation | . NLP is notoriously known as one of the most challenging field in AI because human languages are ever-changing systems defined by complex sets of grammar rules. Languages also come with a variety of exceptions, ambiguities and irregularities that sometimes are not even explicitly formalized in written rules. For these reasons traditional approaches like rule-based programs have always struggled to perform tasks involving human languages. . This is where machine learning and specifically transformer models can help us. . &#10024; Transformers &#10024; . &#11088; A bit of history &#11088; . The transformer architecture was proposed in the seminal paper Attention is all you need by Ashish Vaswani et al. in 2017. The original transformer model was specifically designed to perform machine translation (e.g. translating text from english to french and vice versa), but thanks to its effectiveness and flexibility, in the following years several other architectures have been developed based on this original idea. These transformer-based models represent the state-of-the-art models in most NLP tasks (e.g. summarization and question answering) and recently they have also been successfully employed to tackle computer vision (CV) tasks such as image classification and segmentation. . Over the years, the number of transformer-based models has grown exponentially and it would be pointless trying to learn all of them. However, there are four foundational models that any NLP practitioners should be aware of: . GPT was published by OpenAI in June 2018 and it represented the first pre-trained transformer model. It employs only the decoder part. | . BERT is another pre-trained model published by Google in October 2018. It uses only the encoder part. | . BART and T5 were published in October 2019 by Facebook and Google, respectively. They leverage both the encoder and decoder as the original transformer. | . &#11088; What makes the transformer models work so well &#11088; . Fundamentally, there are two key features that make the transformer architecture so successful compared to other solutions: . transformer models can process sequential data (e.g. words in a sentence) all at once, taking full advantage of the parallelization capabilities offered by GPUs. . | Transformer models incorporate attention layers. These special layers allow the model to pay attention to other sections of the input data when processing a specific point in the sequence. This aspect is extremely relevant in NLP where the meaning of words is heavily affected by the context. . | &#11088; The key components of the original transformer architecture &#11088; . Essentially, the original transformer model is made of two pieces: the encoder and the decoder. . The encoder is a stack of smaller components, called encoder blocks, and each block can be further broken down into two sublayers: . a bidirectional self-attention layer | a feed-forward neural network | . Similarly, the decoder is a stack of decoder blocks, but the internal sublayers are slightly different: . a masked self-attention layer | an encoder-decoder attention layer | a feed-forward neural network | . . &#11088; Three families of transformers &#11088; . One interesting aspect of transformer models is their modular architecture: a transformer does not always require an encoder and a decoder to work properly. For this reason, they can be grouped in three main families based on the components they incorporate: . GPT-like or auto-regressive models. These models use only the decoder part and they are well suited for generation tasks such as text and image generation. | BERT-like or auto-encoding models. These models leverage only the encoder part and they work well for natural language understanding tasks such as text-classification, named entity recognition (i.e. word classification) and extractive question answering. | BART/T5-like or sequence-to-sequence models. These models make use of both the encoder and the decoder. They are designed to perform generative task subject to a certain input such as translation and summarization. | . Now that we have a basic understanding of transformers models and their components, let‚Äôs see how we can leverage one of these pre-trained models to perform sentiment analysis. Along the way, we will explore new concepts and we will dig deeper into some of the ideas we came across in this section. . . &#10024; Fine-tuning a transformer model to recognize emotions &#128514;&#128546;&#128545;&#128515;&#128559; &#10024; . We will fine-tune a transformer model to perform sentiment analysis on tweets. . From the ML perspective, sentiment analysis is just a sentence-classification task. We want to assign the right label to a sentence. . To do that we will use the Hugging Face library which is by far the most popular resource used by practitioners to perform NLP tasks with transformers. On Hugging Face we have access to all sorts of things, from pre-trained models to pre-processed datasets. All of the code is open-source and the API is extremely intuitive. . &#11088; Setup &#11088; . Transformer-based models are fairly large and even though we don&#39;t need to train one from scratch, we still want to take advantage of GPUs. Let&#39;s make sure we have access to one. . !nvidia-smi . Sat Apr 23 23:05:47 2022 +--+ | NVIDIA-SMI 460.32.03 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 42C P0 28W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . Now that we&#39;re sure we have access to a GPU, we can install the Hugging Face libraries: . datasets gives us direct access to the datasets available on Hugging Face. | transformers[sentencepiece] contains the model architectures and the pre-trained parameters. | . !pip install datasets -Uqq !pip install transformers[sentencepiece] -Uqq . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 325 kB 8.0 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 45.5 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77 kB 5.5 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212 kB 51.5 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136 kB 54.4 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127 kB 55.2 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144 kB 53.1 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271 kB 49.0 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94 kB 3.1 MB/s ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.0 MB 7.3 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 50.9 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 45.1 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 40.9 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 37.1 MB/s . This piece of code is just to make sure we all get the same numbers... . import numpy as np import random import torch def set_seeds(seed=1234): &quot;&quot;&quot;Set seeds for reproducibility.&quot;&quot;&quot; np.random.seed(seed) random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) set_seeds(2077) . . &#11088; Loading the data &#11088; . In this example we won&#39;t simply classify tweets as positive or negative, but we will go a step forward, trying to classify them based on the underlying emotion. Thankfully we don&#39;t have to spend time to collect and create our own data because on Hugging Face we can easily access the emotion dataset. From the documentation on the Hugging Face website, we can get some basic information regarding the dataset. It contains English tweets labeled based on six emotions: joy, love, surprise, sadness, anger and fear. . The data is also conveniently divided in a train, validation and test set. . Loading the data in our environment is easy. We just need to import the load_dataset function from the datasets library and pass the name of the data we want as the first parameter. . from datasets import load_dataset dataset = load_dataset(&#39;emotion&#39;) . Using custom data configuration default . Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705... Dataset emotion downloaded and prepared to /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data. . The data is store inside a DatasetDict object which is essentially a dictionary with additional functionalities. Each key-value pair corresponds to a dataset split. In our case we have three splits (train, validation and test) defined as a Dataset object which is another form of dictionariy. . dataset . DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 16000 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 2000 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 2000 }) }) . One handy feature of Dataset objects is that we can explore them as if there were lists. By examining the train split we can see that label contains our target classes (i.e. our six emotions) encoded from 0 to 5 and text contains the tweets that we will feed to the transformer model as input features. . dataset[&#39;train&#39;][:3] . {&#39;label&#39;: [0, 0, 3], &#39;text&#39;: [&#39;i didnt feel humiliated&#39;, &#39;i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake&#39;, &#39;im grabbing a minute to post i feel greedy wrong&#39;]} . dataset[&#39;train&#39;][&#39;text&#39;][:3] . [&#39;i didnt feel humiliated&#39;, &#39;i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake&#39;, &#39;im grabbing a minute to post i feel greedy wrong&#39;] . To figure out which target ID corresponds to which emotion we can access the features attribute to take a look at the datatypes. Our target variable is a ClassLabel object and its names attribute contains the names of the emotions in the correct order. . dataset[&#39;train&#39;].features . {&#39;label&#39;: ClassLabel(num_classes=6, names=[&#39;sadness&#39;, &#39;joy&#39;, &#39;love&#39;, &#39;anger&#39;, &#39;fear&#39;, &#39;surprise&#39;], id=None), &#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)} . dataset[&#39;train&#39;].features[&#39;label&#39;].names . [&#39;sadness&#39;, &#39;joy&#39;, &#39;love&#39;, &#39;anger&#39;, &#39;fear&#39;, &#39;surprise&#39;] . Let&#39;s save both the names and numbers of labels. We can use the label names to create a dictionary mapping the IDs to labels and vice versa. Instead, we will use the number of labels in a few moments to properly define the shape of the output vector of our transformer model. . label_names = dataset[&#39;train&#39;].features[&#39;label&#39;].names num_labels = dataset[&#39;train&#39;].features[&#39;label&#39;].num_classes . ids2labels = {i: label for i, label in enumerate(label_names)} ids2labels . {0: &#39;sadness&#39;, 1: &#39;joy&#39;, 2: &#39;love&#39;, 3: &#39;anger&#39;, 4: &#39;fear&#39;, 5: &#39;surprise&#39;} . Here we are just scratching the surface. Hugging Face DatasetDict and Dataset are extremely powerful classes that offer many other functionalities. . They are also well suited to perform exploratory data analysis (EDA). For the sake of simplicity, we won&#39;t perform any EDA, but when working on a real project, it is a step that we should definitely go through. To learn more about the DatasetDict and Dataset check out the official documentation. . Also keep in mind that there is no standard way to explore datasets. Each dataset has its own peculiarities and this affects how we interact and what we can do with them. That&#39;s why I encourage you, once you have gone through this notebook, to dedicate some time to explore other datasets. Here you can find all the datasets currently available on Hugging Face. The different filters on the left-hand side makes the searching process straightforward. . . &#11088; Taking a bird&#8217;s-eye &#129413; view of the ML pipeline for NLP &#11088; . Now that we have the data, we are ready to move to the actual ML pipeline for NLP tasks. Again, there is no silver bullet, but usually it consists of three steps: . pre-processing phase: unfortunately we cannot feed raw text directly to transformer models. This is the reason why the first step is converting raw text into numbers. We will do that by using a tokenizer. . | training phase: this is where we feed the pre-processed data to the model in order to update the parameters. We will use a pre-trained BERT to build our sentence-classification model. . | post-processing/evaluation phase: in this step we take the model outputs and convert them into a format we can interpret. For our task, we will convert the transformer outputs into probabilities. . | . &#11088; Pre-processing phase &#11088; . Tokenizers are the perfect tools to pre-process raw text. They take a list of sentences as input and return a list of integers plus any other optional output. Tokens are the atomic components of a sentence and depending on the tokenization strategy used they can be words, sub-words or even single characters. A vocabulary (or vocab) is a dictionary mapping tokens to integers and vice versa. The integers in the output list are often called token IDs. . In general the tokenization process involves three steps: . the tokenizer receives the input text and breaks it down into tokens. . | it assigns each token to an integer. . | depending on the transformer model considered, the tokenizer adds special tokens to the vocab. . | ‚ÄºÔ∏è Different transformer models require different special tokens. That&#39;s why it&#39;s paramount to always make sure that the transformer and the tokenizer use the same vocabulary. ‚ÄºÔ∏è . In practice, the first thing we need to do is to load a tokenizer from the Hugging Face library. AutoTokenizer is the generic tokenizer class and it can be instantiated based on a pre-trained model vocabulary using the from_pretrained method and passing the name of the model as argument. As we have just mentioned, we will use a pre-trained BERT model called bert-base-uncased because generally BERT-like models perform very well in sentence-classification tasks. . In most cases, the Hugging Face website provides useful information regarding the pre-trained models. Here we can find the page for our BERT model, while here we can find the full list of pre-trained models available. In the long run, it always pays off to dedicate some time reading the model description in order to familiarize with them. . After defining our AutoTokenizer object, we can call it to get additional information: . vocab_size represents the number of tokens a tokenizer can distinguish. Ours can recognize 30,522 unique tokens. . | model_max_len is an argument related to our transformer model and tells us the maximum length of the input sequences we can feed to the model. The BERT model we will use can accept inputs up to 512 tokens long. . | padding_side and truncation_side are two important concepts that define how the tokenizer will handle inputs of different length when they are processed in batches: . Padding is the process of adding special tokens, called padding tokens, to sequences with fewer tokens. | Truncation is the process of reducing the length of the sequences that are too long. | . To learn more about these two concepts check out the documentation. . | special_tokens, sep_token, pad_token, cls_token and mask_token describe all the extra tokens automatically added by the tokenizer when processing a sentence. These tokens guide the model to properly processing the input data. . | . from transformers import AutoTokenizer checkpoint = &#39;bert-base-uncased&#39; tokenizer = AutoTokenizer.from_pretrained(checkpoint) tokenizer . PreTrainedTokenizerFast(name_or_path=&#39;bert-base-uncased&#39;, vocab_size=30522, model_max_len=512, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens={&#39;unk_token&#39;: &#39;[UNK]&#39;, &#39;sep_token&#39;: &#39;[SEP]&#39;, &#39;pad_token&#39;: &#39;[PAD]&#39;, &#39;cls_token&#39;: &#39;[CLS]&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;}) . Now that we have our tokenizer, let&#39;s see how it works on a few sample sentences. To do that we just need to pass a list of sentences to the tokenizer and it will automatically perform the three steps we mentioned earlier. What we get as output is a dictionary containing three key-value pairs: . input_ids contains the token IDs. In practice, they are the main input data of the transformer model. . | attention_mask defines which other parts of the input sentence the attention layers of the transformer model can attend to when processing a certain position. It&#39;s a list of booleans where 1 means the attention layers can pay attention to that token. There are two common situations where we want the attention layers to ignore certain token IDs: . We are performing a generative task, such as text generation, and we don&#39;t want our model to peek at the tokens it hasn&#39;t generated yet. It would be like cheating! . | We are padding the input text and, since padding tokens don‚Äôt have any semantic meaning, we don‚Äôt want our model to make predictions based on those tokens. . | In our case we get only 1s because we are not trying to generate text and, at least for now, we haven&#39;t padded our sentences. . | Our BERT model can potentially digest a pair of sentences. We can find this information in the model description. token_type_ids tells to the model if a certain token belongs to the first or second sentence. 0 indicates the first sentence and 1 the second one. Since in our example we are not dealing with pairs, we just get lists of 0s. | . sample_text = dataset[&#39;train&#39;][&#39;text&#39;][:3] sample_text . [&#39;i didnt feel humiliated&#39;, &#39;i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake&#39;, &#39;im grabbing a minute to post i feel greedy wrong&#39;] . sample_output = tokenizer(sample_text) sample_output . {&#39;input_ids&#39;: [[101, 1045, 2134, 2102, 2514, 26608, 102], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102], [101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} . Using the convert_ids_to_tokens method we can convert a list of IDs back to tokens. This gives us a chance to get a better understanding of how the tokenizer works. The output list provides two interesting insights: . our tokenizer uses a sub-word tokenization strategy because we can see that the word didnt has been split into didn and ##t. . | it automatically takes care of special tokens. The pre-processing description informs us that the special token [CLS] represents the beginning of a sentence , whereas the [SEP] token is used to separate two paired sentences or signal the end of the input sentence. . | tokenizer.convert_ids_to_tokens(sample_output[&#39;input_ids&#39;][0]) . [&#39;[CLS]&#39;, &#39;i&#39;, &#39;didn&#39;, &#39;##t&#39;, &#39;feel&#39;, &#39;humiliated&#39;, &#39;[SEP]&#39;] . We can also get back the original input data by using the decode method. We need to set the argument skip_special_tokens=True to tell the tokenizer to drop any special tokens. . tokenizer.decode(sample_output[&#39;input_ids&#39;][0], skip_special_tokens=True) . &#39;i didnt feel humiliated&#39; . We can easily pad and truncate our sentences by setting truncation=True and padding=True. To learn more about the different strategies available check the documentation. . Now that we have familiarized ourselves with the tokenizer, we are ready to apply it to our three datasets. The most intuitive way is to pass the text data of each split to the tokenizer. More specifically, this approach can be broken down into 5 steps: . we create an empty DatasetDict object to collect both the raw and tokenized data. . | we pass the sentences of each split to the tokenizer. As we have just mentioned, we set truncation=True and padding=True to make sure the tokenized sequences have the same length. . | we convert the output dictionary into a Dataset object. . | we use the concatenate_datasets function from the datasets library to concatenate the raw and tokenized data together. This step is totally optional. There might be cases where we don&#39;t need to retain the original data. . | we repeat step 2, 3 and 4 for all the remaining splits in our DatasetDict. . | The final output of this process is a DatasetDict object containing for each split the raw and tokenized data. . # ref: https://discuss.huggingface.co/t/add-new-column-to-a-dataset/12149 from datasets import DatasetDict, Dataset, concatenate_datasets # instantiace a empty DatasetDict object tokenized_datasets = DatasetDict() for split in dataset.keys(): # apply tokenizer to dataset split. It return a dictionary tokenized_split = tokenizer(dataset[split][&#39;text&#39;], truncation=True, padding=True) # convert dictionary into a Dataset object tokenized_split = Dataset.from_dict(tokenized_split) # add row data and tokenizer outputs to the DatasetDict object tokenized_datasets[split] = concatenate_datasets([dataset[split], tokenized_split], axis=1) . tokenized_datasets . DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 16000 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) }) . Let&#39;s double check that our tokenized sentences have the same length. . samples = tokenized_datasets[&#39;train&#39;][:10] samples = {k: v for k, v in samples.items()} [len(x) for x in samples[&#39;input_ids&#39;]] . [87, 87, 87, 87, 87, 87, 87, 87, 87, 87] . The process just described is intuitive and it works perfectly fine in most situations. However, it doesn&#39;t scale well with very large datasets. That&#39;s why we will explore an alternative approach which at the beginning might feel more convoluted, but in the long run will provide more flexibility and capabilities. This second approach revolves around two key concepts: mapping and dynamic padding . The DatasetDict class has a map method that enables us to apply a function to each dataset in the object. This function takes a dictionary as input and returns a dictionary as output. In our specific case, the function will apply the tokenizer to our three datasets. The map method is designed to automatically add the key-value pairs of the output dictionary created by the tokenizer to the pre-existing dictionary. That&#39;s why we no longer need to concatenate the data. To get more details about the map method check the documentation. . ‚ÄºÔ∏è Keep in mind that if a key already exists, the new values will replace the existing ones. ‚ÄºÔ∏è . Dynamic padding is a separate step after the tokenization and in order to work properly it requires non padded data. That&#39;s why in the tokenization step we only apply truncation. . In addition, we set batched=True to provide a batch of data to the function to speed up the process. . def get_tokens(split): return tokenizer(split[&#39;text&#39;], truncation=True) # lambda version # tokenized_datasets = dataset.map(lambda split: tokenizer(split[&#39;text&#39;], truncation=True), batched=True) tokenized_datasets = dataset.map(get_tokens, batched=True) tokenized_datasets . DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 16000 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) }) . By default when we set padding=True the tokenizer pads all sentences to match the longest sentence in the entire dataset. Conversely, with dynamic padding we pad our data dynamically when forming the batches based on the longest sequence in each batch. As a result, the padded sequences in a specific batch have the same length, but they are likely to have a different one when compared to padded sequences in another batch. In most cases dynamic padding speeds up training compared to normal padding because we avoid passing to the model over-long sequences containing uninfortmative pad tokens. . To apply dynamic padding we just need to import the class DataCollatorWithPadding from the transformers library and pass our tokenzier as argument. During training, the data collator will be responsible to create batches and dynamically pad the input sequences accordingly. . Let&#39;s see how it works on a sample. . As we can see, since we didn&#39;t apply padding, our sequences in the sample have varying lengths after the tokenization. The next step consists in passing the sample to the DataCollatorWithPadding object to pad the input data based on the longest sequence in the sample, which in this case is 30 tokens. As expected, if we try to apply the data collator on a different sample we get a different maximum length (44 tokens). . samples = tokenized_datasets[&#39;train&#39;][:10] samples = {k: v for k, v in samples.items() if k not in [&#39;text&#39;, &#39;label&#39;]} [len(x) for x in samples[&#39;input_ids&#39;]] . [7, 23, 12, 22, 8, 17, 30, 20, 25, 6] . from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding(tokenizer=tokenizer) . batch = data_collator(samples) {k: v.shape for k, v in batch.items()} . {&#39;attention_mask&#39;: torch.Size([10, 30]), &#39;input_ids&#39;: torch.Size([10, 30]), &#39;token_type_ids&#39;: torch.Size([10, 30])} . samples = tokenized_datasets[&#39;train&#39;][10:20] samples = {k: v for k, v in samples.items() if k not in [&#39;text&#39;, &#39;label&#39;]} print([len(x) for x in samples[&#39;input_ids&#39;]], &#39; n&#39;) batch = data_collator(samples) {k: v.shape for k, v in batch.items()} . [16, 23, 14, 10, 44, 12, 9, 10, 25, 19] . {&#39;attention_mask&#39;: torch.Size([10, 44]), &#39;input_ids&#39;: torch.Size([10, 44]), &#39;token_type_ids&#39;: torch.Size([10, 44])} . We have finally come to the end of the pre-processing phase and we are ready to move to the training phase. . . &#11088; Training phase &#11088; . The training phase with the Hugging Face library can be deconstructed into four steps:1. define the model2. specify the training arguments . define the training loop | Fine-tune (or train for scratch) the model | Since our objective is to classify sentences based on their semantic meaning we need to import the AutoModelForSequenceClassification from the transformers library specifying the checkpoint and the number of output classes. The BERT checkpoint we‚Äôre trying to use was pre-trained to perform a fill-mask and a next-sentence-prediction task. When we import AutoModelForSequenceClassification we are telling the library we want to perform sequence-classification and, as a consequence, it automatically replaces the original model head with a new one specifically designed for our down-stream task. The term head refers to the last layer (or the last few layers) in the model architecture and is responsible to project the transformer outputs onto the proper vector space. In other words, we can adapt a pre-trained transformer model to perform any task by simply changing its head and leaving all the rest (i.e. the backbone) as it is. However, when we change head its parameters are initialized with random vaues. That‚Äôs why we get a warning suggesting us to train the model to be able to use it for predictions and inference. For this specific BERT model we can actually take a look at the new head using the classifier attribute. As we can see, it&#39;s just a linear layer mapping the 768-dimensional vectors generated by the last hidden layer to a 6-dimensional vector space. . The transformers library provides classes to perform all sorts of down-stream tasks. You can find them in the documentation. . from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) . Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . model.classifier . Linear(in_features=768, out_features=6, bias=True) . The next step is to define the training arguments. We can do this by importing the TrainingArguments from the transformers library. The only required parameter is the output_dir which specifies the output directory for the model predictions and checkpoints. In our case we provide three optional parameters: . num_train_epochs defines the number of training epochs | evaluation_strategy tells the training script to evaluate the model at the end of each epoch | fp16 gives us the option to use 16-bit (mixed) precision instead of 32-bit to speed up training. | . Check out the documentation to take a look at the full list of optional parameters. . from transformers import TrainingArguments training_args = TrainingArguments( output_dir=&#39;trainer-run#0001&#39;, num_train_epochs=1, evaluation_strategy=&#39;epoch&#39;, fp16=True ) . Actually before we can move to the third step, there is one last bit of pre-processing we need to do: . remove all the unnecessary columns in features. In our case the only column we need to remove is text. | rename the column containing the ground truth classes labels because the training script by default will look for that column to compute the loss. This aspect is mentioned in the documentation. | set the format for every dataset. We set the torch format because we are using Pytorch. | . tokenized_datasets = tokenized_datasets.remove_columns(&#39;text&#39;) tokenized_datasets = tokenized_datasets.rename_column(&#39;label&#39;, &#39;labels&#39;) tokenized_datasets.set_format(&#39;torch&#39;) tokenized_datasets . DatasetDict({ train: Dataset({ features: [&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 16000 }) validation: Dataset({ features: [&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) test: Dataset({ features: [&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) }) . To define our training loop we simply need to import the Trainer class from the transformers library. This class contains a basic training loop which supports all the items we have generated so far: . pre-trained model | training arguments | pre-processed data | data collator | tokenizer | . As always we can learn more about this class by checking out the official documentation. . from transformers import Trainer trainer = Trainer( model, training_args, data_collator=data_collator, train_dataset=tokenized_datasets[&#39;train&#39;], eval_dataset=tokenized_datasets[&#39;validation&#39;], tokenizer=tokenizer ) . Using amp half precision backend . Now that all the pieces are in place, we can finally fine-tune our transformer model by calling the train method. By default the training loop reports back only the training and the validation loss. Unfortunately, from a human perspective those values are not very informative. Instead, we would like to see metrics like accuracy, precision and recall. We will see how we can add those metrics to the training loop in just a moment, but first let&#39;s briefly jump to the evaluation phase where we can see how to evaluate our model and make predictions. . trainer.train() . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 16000 Num Epochs = 1 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 2000 . . [2000/2000 03:44, Epoch 1/1] Epoch Training Loss Validation Loss . 1 | 0.212300 | 0.177955 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to trainer-run#0001/checkpoint-500 Configuration saved in trainer-run#0001/checkpoint-500/config.json Model weights saved in trainer-run#0001/checkpoint-500/pytorch_model.bin tokenizer config file saved in trainer-run#0001/checkpoint-500/tokenizer_config.json Special tokens file saved in trainer-run#0001/checkpoint-500/special_tokens_map.json Saving model checkpoint to trainer-run#0001/checkpoint-1000 Configuration saved in trainer-run#0001/checkpoint-1000/config.json Model weights saved in trainer-run#0001/checkpoint-1000/pytorch_model.bin tokenizer config file saved in trainer-run#0001/checkpoint-1000/tokenizer_config.json Special tokens file saved in trainer-run#0001/checkpoint-1000/special_tokens_map.json Saving model checkpoint to trainer-run#0001/checkpoint-1500 Configuration saved in trainer-run#0001/checkpoint-1500/config.json Model weights saved in trainer-run#0001/checkpoint-1500/pytorch_model.bin tokenizer config file saved in trainer-run#0001/checkpoint-1500/tokenizer_config.json Special tokens file saved in trainer-run#0001/checkpoint-1500/special_tokens_map.json Saving model checkpoint to trainer-run#0001/checkpoint-2000 Configuration saved in trainer-run#0001/checkpoint-2000/config.json Model weights saved in trainer-run#0001/checkpoint-2000/pytorch_model.bin tokenizer config file saved in trainer-run#0001/checkpoint-2000/tokenizer_config.json Special tokens file saved in trainer-run#0001/checkpoint-2000/special_tokens_map.json ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=2000, training_loss=0.38520724105834964, metrics={&#39;train_runtime&#39;: 225.2836, &#39;train_samples_per_second&#39;: 71.022, &#39;train_steps_per_second&#39;: 8.878, &#39;total_flos&#39;: 339429562491168.0, &#39;train_loss&#39;: 0.38520724105834964, &#39;epoch&#39;: 1.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . &#11088; Post-processing/Evaluation phase &#11088; . To evaluate a model is simple. We just need to call the evaluate method and pass a dataset as a parameter. . trainer.evaluate(tokenized_datasets[&#39;test&#39;]) . ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 . . [250/250 00:05] {&#39;epoch&#39;: 1.0, &#39;eval_loss&#39;: 0.19188867509365082, &#39;eval_runtime&#39;: 5.6257, &#39;eval_samples_per_second&#39;: 355.512, &#39;eval_steps_per_second&#39;: 44.439} . Generating the predictions is slightly more convolutated because we need to apply a bit of post-processing. . In fact, after calling the predict method we get a PredictionOutput object containing three items: . predictions consists of a matrix with a row for each input sequence and a column for each label. In our case, we have 2000 rows and 6 columns, therefore we get a 2000 x 6 matrix. Each row vector contains logits which represent the unnormalized output scores of the transformer model. We can interpret these scores as probabilities by passing them through a softmax function. The actual prediction is the class corresponding to the index with the largest logit (or probability). | label_ids contains the ground truth labels | metrics contains the metrics. For the moment we have only the loss. | . predictions = trainer.predict(tokenized_datasets[&#39;test&#39;]) predictions . ***** Running Prediction ***** Num examples = 2000 Batch size = 8 . . [250/250 00:21] PredictionOutput(predictions=array([[ 7.215 , -1.412 , -1.734 , -0.984 , -1.61 , -1.484 ], [ 7.285 , -1.78 , -1.742 , -1.078 , -1.317 , -1.413 ], [ 7.223 , -1.898 , -1.807 , -1.092 , -1.128 , -1.452 ], ..., [-2.326 , 6.58 , -0.3184 , -1.887 , -1.687 , -1.321 ], [-2.297 , 6.38 , -0.06232, -2.316 , -1.183 , -1.547 ], [-2.316 , -0.9844 , -1.435 , -2.07 , 4.11 , 3.473 ]], dtype=float16), label_ids=array([0, 0, 0, ..., 1, 1, 4]), metrics={&#39;test_loss&#39;: 0.19188867509365082, &#39;test_runtime&#39;: 10.162, &#39;test_samples_per_second&#39;: 196.811, &#39;test_steps_per_second&#39;: 24.601}) . predictions.predictions.shape, predictions.label_ids.shape . ((2000, 6), (2000,)) . In practice, to turn logits into probabilities we import the softmax function from torch.nn.functional and pass the logit scores as torch.Tensor. We need to set dim=-1 to make sure the softmax function is applied across columns and not rows. To get the index of the largest logit/probability (i.e. the actual prediction) we can use the argmax function available in numpy. Again, we need to specify axis=-1 to apply the function across columns. . import numpy as np import torch from torch.nn.functional import softmax logits = predictions.predictions # targets = predictions.label_ids probs = softmax(torch.Tensor(logits), dim=-1) preds = np.argmax(logits, axis=-1) . probs . tensor([[9.9910e-01, 1.7905e-04, 1.2972e-04, 2.7476e-04, 1.4685e-04, 1.6657e-04], [9.9918e-01, 1.1550e-04, 1.1998e-04, 2.3309e-04, 1.8349e-04, 1.6674e-04], [9.9912e-01, 1.0924e-04, 1.1974e-04, 2.4474e-04, 2.3605e-04, 1.7069e-04], ..., [1.3553e-04, 9.9802e-01, 1.0093e-03, 2.1033e-04, 2.5695e-04, 3.7023e-04], [1.7019e-04, 9.9719e-01, 1.5900e-03, 1.6690e-04, 5.1862e-04, 3.6029e-04], [1.0496e-03, 3.9766e-03, 2.5351e-03, 1.3424e-03, 6.4819e-01, 3.4291e-01]]) . preds . array([0, 0, 0, ..., 1, 1, 4]) . This is the end of this section. . In the next one we will explore few different way to improve the current pipeline, including how to: . pass more meaningful metrics to the training loop | save the model checkpoint locally or on a cloud storage | evaluate the model through more elegant visualizations | . . &#11088; Improving the pipeline &#11088; . Adding metrics . Now that we know how to evaluate our model and generate predictions, first nice addition would be the possibility to look at more meaningful metrics. To do that first we need to import the load_metric function from the datasets library and pass the name of the metric we want as an argument. Each metric takes as input the predicted and the ground truth values plus any additional metric-specific argument. Since we are dealing with a classification task, we add accuracy, precision, recall and F1 score to the training procedure. . We can check the full list of metrics available using the list_metrics function from the datasets library. . from datasets import load_metric accuracy_metric = load_metric(&#39;accuracy&#39;) precision_metric = load_metric(&#39;precision&#39;) recall_metric = load_metric(&#39;recall&#39;) f1_metric = load_metric(&#39;f1&#39;) . print(accuracy_metric.compute(predictions=preds, references=predictions.label_ids)) print(precision_metric.compute(predictions=preds, references=predictions.label_ids, average=&#39;weighted&#39;)) print(recall_metric.compute(predictions=preds, references=predictions.label_ids, average=&#39;weighted&#39;)) print(f1_metric.compute(predictions=preds, references=predictions.label_ids, average=&#39;weighted&#39;)) . {&#39;accuracy&#39;: 0.929} {&#39;precision&#39;: 0.928007871781864} {&#39;recall&#39;: 0.929} {&#39;f1&#39;: 0.9283332983256687} . from datasets import list_metrics list_metrics() . The Trainer class has an ad hoc parameter called compute_metrics that enables us to pass a function to compute the metrics during the evaluation phase. This function takes a PredictionOutput object as input and returns a dictionary containing the metric values. . Let&#39;s define this function and manually test it on the predictions we got a few moments ago. . def compute_metrics_fn(eval_preds): metrics = dict() accuracy_metric = load_metric(&#39;accuracy&#39;) precision_metric = load_metric(&#39;precision&#39;) recall_metric = load_metric(&#39;recall&#39;) f1_metric = load_metric(&#39;f1&#39;) logits = eval_preds.predictions labels = eval_preds.label_ids preds = np.argmax(logits, axis=-1) metrics.update(accuracy_metric.compute(predictions=preds, references=labels)) metrics.update(precision_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) metrics.update(recall_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) metrics.update(f1_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) return metrics . metrics = compute_metrics_fn(predictions) metrics . {&#39;accuracy&#39;: 0.929, &#39;f1&#39;: 0.9283332983256687, &#39;precision&#39;: 0.928007871781864, &#39;recall&#39;: 0.929} . At a first glance, it seems our compute_metrics_fn works as expected. Let&#39;s run again the training loop with just a few changes: . train for 3 epochs, rather than just 1 | pass larger batches to the model by setting the per_device_train_batch_size argument | keep the best model by setting load_best_model_at_end=True | compute the metrics by passing the function we have just created to the training loop through the compute_metrics argument. | . model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) # define training arguments training_args = TrainingArguments( &#39;trainer-run#0002&#39;, evaluation_strategy=&#39;epoch&#39;, save_strategy=&#39;epoch&#39;, num_train_epochs=3, per_device_train_batch_size=16, fp16=True ) . trainer = Trainer( model, training_args, train_dataset=tokenized_datasets[&#39;train&#39;], eval_dataset=tokenized_datasets[&#39;validation&#39;], data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics_fn, ) . Using amp half precision backend . trainer.train() . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 16000 Num Epochs = 3 Instantaneous batch size per device = 16 Total train batch size (w. parallel, distributed &amp; accumulation) = 16 Gradient Accumulation steps = 1 Total optimization steps = 3000 . . [3000/3000 07:26, Epoch 3/3] Epoch Training Loss Validation Loss Accuracy Precision Recall F1 . 1 | 0.226400 | 0.199119 | 0.935000 | 0.940978 | 0.935000 | 0.935969 | . 2 | 0.119400 | 0.132668 | 0.935000 | 0.935721 | 0.935000 | 0.934792 | . 3 | 0.078700 | 0.173672 | 0.938500 | 0.940526 | 0.938500 | 0.938868 | . &lt;/div&gt; &lt;/div&gt; ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 Saving model checkpoint to trainer-run#0002/checkpoint-1000 Configuration saved in trainer-run#0002/checkpoint-1000/config.json Model weights saved in trainer-run#0002/checkpoint-1000/pytorch_model.bin tokenizer config file saved in trainer-run#0002/checkpoint-1000/tokenizer_config.json Special tokens file saved in trainer-run#0002/checkpoint-1000/special_tokens_map.json ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 Saving model checkpoint to trainer-run#0002/checkpoint-2000 Configuration saved in trainer-run#0002/checkpoint-2000/config.json Model weights saved in trainer-run#0002/checkpoint-2000/pytorch_model.bin tokenizer config file saved in trainer-run#0002/checkpoint-2000/tokenizer_config.json Special tokens file saved in trainer-run#0002/checkpoint-2000/special_tokens_map.json ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 Saving model checkpoint to trainer-run#0002/checkpoint-3000 Configuration saved in trainer-run#0002/checkpoint-3000/config.json Model weights saved in trainer-run#0002/checkpoint-3000/pytorch_model.bin tokenizer config file saved in trainer-run#0002/checkpoint-3000/tokenizer_config.json Special tokens file saved in trainer-run#0002/checkpoint-3000/special_tokens_map.json Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=3000, training_loss=0.2011304931640625, metrics={&#39;train_runtime&#39;: 446.3705, &#39;train_samples_per_second&#39;: 107.534, &#39;train_steps_per_second&#39;: 6.721, &#39;total_flos&#39;: 1161520801033536.0, &#39;train_loss&#39;: 0.2011304931640625, &#39;epoch&#39;: 3.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; trainer.evaluate(tokenized_datasets[&#39;test&#39;]) . ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 . . [250/250 00:06] {&#39;epoch&#39;: 3.0, &#39;eval_accuracy&#39;: 0.93, &#39;eval_f1&#39;: 0.9304119781172865, &#39;eval_loss&#39;: 0.18988770246505737, &#39;eval_precision&#39;: 0.9323717575998934, &#39;eval_recall&#39;: 0.93, &#39;eval_runtime&#39;: 7.6352, &#39;eval_samples_per_second&#39;: 261.943, &#39;eval_steps_per_second&#39;: 32.743} . WOW!ü•Ç After training for just 3 epochs we reach almost 95% in accuracy on the test set and the other metrics look good. . Let‚Äôs define a function to automatically convert logits into label IDs.Then we import the classification_report and ConfusionMatrixDisplay functions from sklearn.metrics and pass the ground truth and predicted labels to those two functions. The classification_report function generates a detailed report providing additional information for each label, whereas the ConfusionMatrixDisplay function returns a neat confusion matrix showing where our sentence-classification model performs well and where it falls short. For instance, by looking at the numbers in the confusion matrix, it is apparent that the model sometimes mixes up joy and love and misinterprets fear as surprise. . def get_predictions(dataset, model): raw_preds = model.predict(dataset) logits = raw_preds.predictions targets = raw_preds.label_ids preds = np.argmax(logits, axis=-1) return (targets, preds) # get indexes and target labels targets, preds = get_predictions(tokenized_datasets[&#39;test&#39;], trainer) . ***** Running Prediction ***** Num examples = 2000 Batch size = 8 . . [250/250 04:22] from sklearn.metrics import classification_report print(classification_report(targets, preds, target_names=label_names)) . precision recall f1-score support sadness 0.96 0.97 0.97 581 joy 0.97 0.93 0.95 695 love 0.79 0.92 0.85 159 anger 0.93 0.91 0.92 275 fear 0.87 0.91 0.89 224 surprise 0.79 0.70 0.74 66 accuracy 0.93 2000 macro avg 0.89 0.89 0.89 2000 weighted avg 0.93 0.93 0.93 2000 . from sklearn.metrics import ConfusionMatrixDisplay from matplotlib import pyplot as plt fig, ax = plt.subplots(dpi=100) ConfusionMatrixDisplay.from_predictions(targets, preds, display_labels=label_names, cmap=&#39;GnBu&#39;, ax=ax ) plt.show() . Saving model checkpoint . This few bits of code show how to save a model checkpoint locally or on the cloud: . we zip the checkpoint folder we want to save using shutil.make_archive. The first argument is the name of the file to create, the second is the format and the third is the input directory. | we use shutil.copy to copy the zip folder to another location. In this example, we copy the checkpoint on Google Drive. | . ‚ÄºÔ∏è Make sure to adjust the base_name, root_dir, src and dst parameters accordingly to the environment used. ‚ÄºÔ∏è . import shutil shutil.make_archive(base_name=&#39;/content/last_checkpoint&#39;, format=&#39;zip&#39;, root_dir=&#39;/content/trainer-run#0002/checkpoint-2000&#39;) . &#39;/content/last_checkpoint.zip&#39; . shutil.copy(src=&#39;/content/drive/MyDrive/transformers/apr24_last_checkpoint.zip&#39;, dst=&#39;/content/last_checkpoint.zip&#39;) . . &#128218; Additional resources . The Hugging Face course teaches additional topics we didn&#39;t cover in this notebook. | If you prefer books, Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf is the perfect resource to learn more about NLP and transformers. | The Illustrated Transformer and The Illustrated BERT by Jay Alammar are two great blog posts full of amazing visualizations to further explore transformers. | The Annotated Transformer by Harvard NLP goes through the original paper &quot;Attention is All You Need&quot; showing how to implement the transformer architecture in PyTorch line by line. | &lt;/div&gt; .",
            "url": "https://matteopilotto.github.io/blog/2022/04/24/hg_transformer_finetune_part1.html",
            "relUrl": "/2022/04/24/hg_transformer_finetune_part1.html",
            "date": " ‚Ä¢ Apr 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://matteopilotto.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://matteopilotto.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://matteopilotto.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://matteopilotto.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}