{
  
    
        "post0": {
            "title": "Image classification with Google's ViT and Hugging Face ü§ó",
            "content": ". &#128278; Learning objectives . In A brief introduction to Vision Transfomer (ViT) | In Setup | In Loading the data | In EDA with pandas | In Image pre-processing | In Training | In Evaluation | In Data augmentation | In Conclusion | In Acknowledgments | In Additional resources | . &#10024; A brief introduction to Vision Transfomer (ViT) &#10024; . The Vision Transformer (ViT) is a transformer-based architecture for computer vision tasks proposed in the An Image is Worth 16x16 Words paper by researchers at Google in 2020. The ViT leverages only the encoder part of the original Transformer architecture and each input image is interpreted as a sequence of patch embeddings. . In the pre-processing phase, an image is broken down into a grid of square patches. Each patch is flattened and linearly projected onto a lower-dimensional space. A learnable position embedding is then added to the resulting patch embedding to retain the positional information. These steps are applied at once to each patch forming the image. . Before feeding the sequence of patch embeddings and position embeddings to the encoder, an extra learnable &quot;class&quot; embedding is prepended to the sequence. The last hidden state of this embedding serves as representation of the overall image. A classification head on top of the transformer encoder maps the last hidden state of the class embedding onto the final vector space representing the labels. . In this notebook we will learn how to take a ViT model pre-trained on a large dataset and fine-tune it to perform image classification on a smaller task-specific dataset. We will use the google/vit-base-patch16-224-in21k as a pre-trained model. This checkpoint was generated by pre-training a ViT on ImageNet-21k at a resolution of 224x224. During pre-processing images were converted into sequences of 16x16 patches. And as for the data, we will use the Matthijs/snacks dataset which consists of 6745 images and 20 classes. . . . &#10024; Setup &#10024; . We make sure our instance has a GPU and pip install the Hugging Face libraries. . !nvidia-smi . Sun May 15 21:24:57 2022 +--+ | NVIDIA-SMI 460.32.03 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 27W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . !pip install datasets -Uqq !pip install transformers[sentencepiece] -Uqq . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342 kB 7.7 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212 kB 39.1 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 3.4 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 45.5 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136 kB 57.3 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127 kB 54.4 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271 kB 53.3 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144 kB 53.4 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94 kB 2.1 MB/s ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.2 MB 6.1 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 42.5 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 33.1 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 48.9 MB/s . import numpy as np import random import torch def set_seeds(seed=1234): &quot;&quot;&quot;Set seeds for reproducibility.&quot;&quot;&quot; np.random.seed(seed) random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) set_seeds(2077) . . &#10024; Loading the data &#10024; . We can conveniently get access to the dataset using the load_dataset function from the datasets library. . The resulting DatasetDict object already contains a train, validation and test set as Dataset objects. Each dataset has two features: image and label. . Images are PIL.JpegImagePlugin.JpegImageFile objects, whereas labels are simple integers. . We can inspect the content of a Dataset as if we were inspecting a normal list. . from datasets import load_dataset raw_data = load_dataset(&#39;Matthijs/snacks&#39;) raw_data . Downloading and preparing dataset snacks/default to /root/.cache/huggingface/datasets/Matthijs___snacks/default/0.0.1/c0ce49075aa469a098a5f2e3455941c894e02e1c9bf642d4d33e6c51460ff590... Dataset snacks downloaded and prepared to /root/.cache/huggingface/datasets/Matthijs___snacks/default/0.0.1/c0ce49075aa469a098a5f2e3455941c894e02e1c9bf642d4d33e6c51460ff590. Subsequent calls will reuse this data. . DatasetDict({ train: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 4838 }) test: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 952 }) validation: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 955 }) }) . raw_data[&#39;train&#39;][:3] . {&#39;image&#39;: [&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x7FF799067450&gt;, &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=383x256 at 0x7FF799067A10&gt;, &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x256 at 0x7FF70BFC9490&gt;], &#39;label&#39;: [5, 5, 5]} . We can get the full list of labels through the features attribute. They are represented as a ClassLabel object. . Each of the 20 labels represent a kind of snack... Some are healthier than others. üç≠ . labels = raw_data[&#39;train&#39;].features[&#39;label&#39;] labels . ClassLabel(num_classes=20, names=[&#39;apple&#39;, &#39;banana&#39;, &#39;cake&#39;, &#39;candy&#39;, &#39;carrot&#39;, &#39;cookie&#39;, &#39;doughnut&#39;, &#39;grape&#39;, &#39;hot dog&#39;, &#39;ice cream&#39;, &#39;juice&#39;, &#39;muffin&#39;, &#39;orange&#39;, &#39;pineapple&#39;, &#39;popcorn&#39;, &#39;pretzel&#39;, &#39;salad&#39;, &#39;strawberry&#39;, &#39;waffle&#39;, &#39;watermelon&#39;], id=None) . Let&#39;s visualize some of the images with matplotlib to get a better understanding of the data. The ClassLabel class has a int2str method that allows us to map the IDs to the actual labels. . import matplotlib.pyplot as plt nrows = 2 ncols = 5 plt.figure(figsize=(ncols*2, nrows*2), dpi=100) for i in range(1, nrows*ncols + 1): sample_image = raw_data[&#39;train&#39;][i] plt.subplot(nrows, ncols, i) plt.imshow(sample_image[&#39;image&#39;].resize((128, 128))) plt.title(labels.int2str(sample_image[&#39;label&#39;]), fontsize=10) plt.axis(&#39;off&#39;) plt.show() . Probably the reason why we visualized images representing only a single class is because the data is ordered by labels. . Let&#39;s plot some images once again, but this time using a random sample. . random_sample = raw_data[&#39;train&#39;].shuffle(seed=2077).select(range(100)) . nrows = 2 ncols = 5 plt.figure(figsize=(ncols*2, nrows*2), dpi=100) for i in range(nrows*ncols): sample_image = random_sample[i] plt.subplot(nrows, ncols, i+1) plt.imshow(sample_image[&#39;image&#39;].resize((128, 128))) plt.title(labels.int2str(sample_image[&#39;label&#39;]), fontsize=10) plt.axis(&#39;off&#39;) plt.show() . Now we&#39;re talking! . . &#10024; EDA with pandas &#10024; . With Hugging Face DatasetDict and Dataset classes we can convert the data into a pandas.DataFrame object to investigate its content leveraging our knowledge of the pandas library. . We just need to change the output format with the set_format method and grab the entire dataset by slicing the entire dataset with [:]. . raw_data.set_format(&#39;pandas&#39;) raw_data_pd = raw_data[&#39;train&#39;][:] raw_data_pd.shape . (4838, 2) . We count the number of images available for each label to check if there is any imbalance. Then we visualize the result as a barplot using seaborn. . samples_count = ( raw_data_pd .groupby(&#39;label&#39;) .count() .reset_index() .rename(columns={&#39;image&#39;: &#39;count&#39;}) # .reset_index() ) # map IDs to labels samples_count[&#39;label&#39;] = samples_count[&#39;label&#39;].map(lambda x: labels.int2str(x)) samples_count . label count . 0 apple | 250 | . 1 banana | 250 | . 2 cake | 249 | . 3 candy | 249 | . 4 carrot | 249 | . 5 cookie | 249 | . 6 doughnut | 250 | . 7 grape | 250 | . 8 hot dog | 250 | . 9 ice cream | 250 | . 10 juice | 250 | . 11 muffin | 250 | . 12 orange | 249 | . 13 pineapple | 260 | . 14 popcorn | 180 | . 15 pretzel | 154 | . 16 salad | 250 | . 17 strawberry | 249 | . 18 waffle | 250 | . 19 watermelon | 250 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; import seaborn as sns sns.set_theme(style=&#39;whitegrid&#39;, palette=&#39;pastel&#39;) plt.figure(dpi=100) bar = sns.barplot(y=&#39;label&#39;, x=&#39;count&#39;, data=samples_count, orient=&#39;h&#39;) plt.show() . As we can observe from the table and bar plot above most classes are represented by about 250 images. pineapple is the most common class with 260 images, whereas pretzel is the least represented class with only 154 images. . Overall, we can say this dataset is pretty well balanced. . Before moving to the next section, let&#39;s make sure to restore the original format of the data with the reset_format method. . raw_data.reset_format() . . &#10024; Image pre-processing &#10024; . As for text, images also require some sort of pre-processing before they can be fed to a ViT. The most basic approach just requires resizing and normalizing the images across the channels with the same statistics used during pre-training. . We will see how to increase the robustness of our model with the help of data augmentation in a section below. . The Hugging Face library provides a ViTFeatureExtractor class that automatically performs resizing and normalization according to the pre-training statistics. . To load the pre-processing pipeline, sometimes referred as feature extractor, we simply pass the google/vit-base-patch16-224-in21k checkpoint as an argument to the from_pretrained method when instantiating the ViTFeatureExtractor object. . It specifies that images need to be resized at a resolution of 224x224 and normalized across channels using image_mean and image_std statistics. . from transformers import ViTFeatureExtractor checkpoint = &#39;google/vit-base-patch16-224-in21k&#39; feature_extractor = ViTFeatureExtractor.from_pretrained(checkpoint) feature_extractor . ViTFeatureExtractor { &#34;do_normalize&#34;: true, &#34;do_resize&#34;: true, &#34;feature_extractor_type&#34;: &#34;ViTFeatureExtractor&#34;, &#34;image_mean&#34;: [ 0.5, 0.5, 0.5 ], &#34;image_std&#34;: [ 0.5, 0.5, 0.5 ], &#34;resample&#34;: 2, &#34;size&#34;: 224 } . We can test the feature extractor by simply passing a sample image as an argument. . What we get in return is a dictionary with a pixel_values key and associated value is a list that contains a numpy.ndarray of shape 3x224x224 representing the pre-processed image. . sample_image = random_sample[&#39;image&#39;][0] sample_image . sample_image_features = feature_extractor(sample_image) sample_image_features . {&#39;pixel_values&#39;: [array([[[-0.5294118 , -0.5294118 , -0.5137255 , ..., 0.18431377, 0.16078436, -0.03529412], [-0.54509807, -0.5529412 , -0.5372549 , ..., -0.00392157, 0.12156868, 0.12941182], [-0.56078434, -0.5686275 , -0.5686275 , ..., -0.05882353, 0.05098045, 0.11372554], ..., [-0.30196077, -0.19215685, 0.07450986, ..., 0.37254906, 0.2941177 , 0.18431377], [-0.27843136, -0.23137254, 0.15294123, ..., 0.41176474, 0.2941177 , 0.3176471 ], [-0.23921567, -0.25490195, -0.01960784, ..., 0.5921569 , 0.30980396, 0.37254906]], [[-0.19215685, -0.19215685, -0.19215685, ..., 0.8039216 , 0.75686276, 0.5372549 ], [-0.20784312, -0.21568626, -0.21568626, ..., 0.5529412 , 0.654902 , 0.64705884], [-0.23137254, -0.24705881, -0.24705881, ..., 0.39607847, 0.48235297, 0.5294118 ], ..., [ 0.05882359, 0.15294123, 0.39607847, ..., 0.58431375, 0.5058824 , 0.39607847], [ 0.09019613, 0.12156868, 0.47450984, ..., 0.62352943, 0.5058824 , 0.5294118 ], [ 0.12156868, 0.09803927, 0.30980396, ..., 0.7882353 , 0.52156866, 0.58431375]], [[-0.5294118 , -0.5294118 , -0.52156866, ..., 0.6156863 , 0.6 , 0.4039216 ], [-0.54509807, -0.5529412 , -0.54509807, ..., 0.3803922 , 0.5058824 , 0.5294118 ], [-0.56078434, -0.5764706 , -0.5764706 , ..., 0.23921573, 0.34901965, 0.427451 ], ..., [-0.27843136, -0.19215685, 0.04313731, ..., 0.6392157 , 0.56078434, 0.45098042], [-0.23921567, -0.20784312, 0.12941182, ..., 0.6862745 , 0.5764706 , 0.5921569 ], [-0.18431371, -0.2235294 , -0.02745098, ..., 0.8352941 , 0.5921569 , 0.654902 ]]], dtype=float32)]} . sample_image_features[&#39;pixel_values&#39;][0].shape . (3, 224, 224) . We can visually inspect the pre-processed image by first converting the numpy.ndarray into a torch.Tensor and then convert the tensor into a PIL.Image.Image with the help of the transforms.ToPILImage() from the torchvision library. . from torchvision import transforms import torch transforms.ToPILImage()(torch.tensor(sample_image_features[&#39;pixel_values&#39;][0])) . Now that we have a basic grasp of how the ViT feature extractor works, we can define a function that extract the pixel_values from each data point and apply this function on-the-fly on batches with the set_transform method everytime __getitem__ is called. . We can use the deepcopy function from the copy library to duplicate the datasets and keep the original ones unaltered. . def apply_feature_extractor(examples): examples[&#39;pixel_values&#39;] = [torch.tensor(feature_extractor(image.convert(&#39;RGB&#39;))[&#39;pixel_values&#39;][0]) for image in examples[&#39;image&#39;]] return examples . import copy data_preprocessed = copy.deepcopy(raw_data) data_preprocessed.set_transform(apply_feature_extractor) data_preprocessed . DatasetDict({ train: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 4838 }) test: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 952 }) validation: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 955 }) }) . To see the function at work we simply need to access a sample. . data_preprocessed[&#39;train&#39;][0] . {&#39;image&#39;: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x7FF700ABDE50&gt;, &#39;label&#39;: 5, &#39;pixel_values&#39;: tensor([[[ 0.0039, 0.0275, 0.0275, ..., 0.7412, 0.7569, 0.7020], [ 0.0196, -0.0196, -0.0039, ..., 0.7569, 0.7490, 0.6863], [-0.0431, 0.0980, 0.3176, ..., 0.7490, 0.6549, 0.5608], ..., [-0.3333, -0.3255, -0.3098, ..., -0.3020, -0.3647, -0.3725], [-0.3490, -0.3255, -0.3020, ..., -0.3176, -0.3961, -0.3961], [-0.4118, -0.3569, -0.3176, ..., -0.2941, -0.3647, -0.3569]], [[-0.3647, -0.3333, -0.3176, ..., 0.7255, 0.7882, 0.7569], [-0.2863, -0.3255, -0.3020, ..., 0.6784, 0.7098, 0.6863], [-0.2627, -0.1137, 0.1137, ..., 0.5843, 0.5294, 0.4667], ..., [-0.4824, -0.4902, -0.4824, ..., -0.6627, -0.6863, -0.6784], [-0.4980, -0.4745, -0.4667, ..., -0.6627, -0.6941, -0.6863], [-0.5529, -0.4980, -0.4745, ..., -0.6314, -0.6549, -0.6314]], [[-0.3961, -0.3804, -0.3647, ..., 0.5294, 0.5529, 0.5216], [-0.3412, -0.3725, -0.3490, ..., 0.4980, 0.5059, 0.4588], [-0.3412, -0.1843, 0.0510, ..., 0.4353, 0.3569, 0.2706], ..., [-0.1608, -0.1686, -0.1765, ..., -0.6078, -0.6314, -0.6157], [-0.1529, -0.1529, -0.1608, ..., -0.6157, -0.6471, -0.6314], [-0.2078, -0.1765, -0.1686, ..., -0.6000, -0.6235, -0.5843]]])} . It seems to work properly. Everytime one or more samples are accessed, the pre-processing function is invoked to generate the pixel_values which are then added to the features. . Just to be 100% sure the pre-processing transformation is working in the right way, let&#39;s visualize a couple of images. . nrows = 2 ncols = 5 plt.figure(figsize=(ncols*2, nrows*2), dpi=100) preprocessed_samples = data_preprocessed[&#39;train&#39;].shuffle(seed=2077).select(range(nrows * ncols)) for i in range(nrows*ncols): preprocessed_sample = preprocessed_samples[i] plt.subplot(nrows, ncols, i+1) plt.imshow(transforms.ToPILImage()(torch.tensor(preprocessed_sample[&#39;pixel_values&#39;])).resize((128, 128))) plt.title(labels.int2str(preprocessed_sample[&#39;label&#39;]), fontsize=10) plt.axis(&#39;off&#39;) plt.show() . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). del sys.path[0] . It&#39;s looking good! . Before moving to the training phase, there is one last step we have to do: rename the label feature as labels because by default the model will look for this specific column to compute the loss. . data_cleaned = data_preprocessed.rename_column(&#39;label&#39;, &#39;labels&#39;) data_cleaned . DatasetDict({ train: Dataset({ features: [&#39;image&#39;, &#39;labels&#39;], num_rows: 4838 }) test: Dataset({ features: [&#39;image&#39;, &#39;labels&#39;], num_rows: 952 }) validation: Dataset({ features: [&#39;image&#39;, &#39;labels&#39;], num_rows: 955 }) }) . . &#10024; Training &#10024; . We can get access to a pre-trained ViT model using the ViTForImageClassification class and use its from_pretrained method to load a pre-existing checkpoint. . In addition to the checkpoint, we also need to set num_labels to correctly represent the number of classes in our dataset. In this way, during initialization the ViTForImageClassification class will replace the default head with a new one intended for our down-stream dataset. . For the sake of completeness, in this specific example we also define the id2label and label2id parameters to map IDs to labels and vice versa, but they are totally optional and they are not as relevant as num_labels. . from transformers import ViTForImageClassification model = ViTForImageClassification.from_pretrained( checkpoint, num_labels=labels.num_classes, id2label={index: label for index, label in enumerate(labels.names)}, label2id={label: index for index, label in enumerate(labels.names)} ) . Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: [&#39;pooler.dense.bias&#39;, &#39;pooler.dense.weight&#39;] - This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . By default the training loop reports back only the training and validation loss which from a human perspective can be difficult to interpret. . For this reason, we define a function to compute some of the metrics commonly used in a classification task: accuracy, precision, recall and f1 score. . They are all available through the load_metric function from the datasets library. . We define a function that at each evaluation step will take the validation predictions as input and will report back the metrics we chose. . import numpy as np from datasets import load_metric # define function to compute metrics def compute_metrics_fn(eval_preds): metrics = dict() accuracy_metric = load_metric(&#39;accuracy&#39;) precision_metric = load_metric(&#39;precision&#39;) recall_metric = load_metric(&#39;recall&#39;) f1_metric = load_metric(&#39;f1&#39;) logits = eval_preds.predictions labels = eval_preds.label_ids preds = np.argmax(logits, axis=-1) metrics.update(accuracy_metric.compute(predictions=preds, references=labels)) metrics.update(precision_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) metrics.update(recall_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) metrics.update(f1_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) return metrics . The TrainingArguments class from the transformers library is responsible for gathering up all the parameters to customize the training loop. The only required parameter is an output_dir to save predictions and checkpoints, but in this case we also specified parameters like the number of epochs, the batch size and how frequently we want to evaluate the model. . By default the remove_unused_columns is set to False so that during training the model automatically drops unwanted features like the image items in our dataset. However, we can&#39;t drop this feature yet because the set_transform method and the apply_feature_extractor function we defined earlier require the raw images to generate the pixel_values feature. . That&#39;s why we need to explicitly set this parameter to True. . from transformers import TrainingArguments training_args = TrainingArguments( output_dir=&#39;vit-run#0001&#39;, num_train_epochs=5, per_device_train_batch_size=16, per_device_eval_batch_size=16, save_steps=200, logging_steps=200, evaluation_strategy=&#39;steps&#39;, eval_steps=200, load_best_model_at_end=True, remove_unused_columns=False, fp16=True ) . The data collator is that piece of code responsible to assemble together individual samples into batches. Since we instructed the training loop to keep all the features, we can&#39;t rely on the DefaultDataCollator from the transformers library anymore. During training it would attempt to feed to the model also the original images, but as we well know at this point, the ViT model only accepts pixel_values and labels as input features. . We can overcome the issue by defining our own collate function. During training and inference the collate function receives as input a list of data points. The length of the list is defined by the batch size. . In this specific example, a data point corresponds to a dictionary with three key-value pairs: image, labels and pixel_values. Knowing that, we define our custom collate function so that it only keeps pixel_values and labels and ignores image. . pixel_values are 3D (channels, height, width) tensors and therefore we bundle them together with torch.stack to form a 4D tensor representing batch size, channels, height and width. labels are just integers and it&#39;s sufficient to convert the list containing them into a torch.tensor. . # e.g. examples = [{&#39;image&#39;: , &#39;labels&#39;: , &#39;pixel_values&#39;: }] def collate_fn(examples): pixel_values = torch.stack([example[&#39;pixel_values&#39;] for example in examples]) labels = torch.tensor([example[&#39;labels&#39;] for example in examples]) return {&#39;pixel_values&#39;: pixel_values, &#39;labels&#39;: labels} . We can test the custom collate function by providing a list of examples. . In this case we assume that a batch includes 8 samples. . collate_test = collate_fn([data_cleaned[&#39;train&#39;][i] for i in range(8)]) print(collate_test[&#39;pixel_values&#39;].shape) print(collate_test[&#39;labels&#39;]) . torch.Size([8, 3, 224, 224]) tensor([5, 5, 5, 5, 5, 5, 5, 5]) . It returned what we were expecting and so we&#39;re good to move on! . The Trainer class from the transformers library is the training loop itself. We provide all the items we&#39;ve generated so far: . pre-trained model | training arguments | custom collate function | training and validation dataset | custom metrics function | . from transformers import Trainer trainer = Trainer( model, training_args, data_collator=collate_fn, train_dataset=data_cleaned[&#39;train&#39;], eval_dataset=data_cleaned[&#39;validation&#39;], compute_metrics=compute_metrics_fn ) . Using amp half precision backend . The train method starts the training process. . üí° It is completely fine to get different numbers from one iteration to another. . trainer.train() . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 4838 Num Epochs = 5 Instantaneous batch size per device = 16 Total train batch size (w. parallel, distributed &amp; accumulation) = 16 Gradient Accumulation steps = 1 Total optimization steps = 1515 . . [1515/1515 11:26, Epoch 5/5] Step Training Loss Validation Loss Accuracy Precision Recall F1 . 200 | 2.043600 | 1.224619 | 0.929843 | 0.931073 | 0.929843 | 0.929469 | . 400 | 0.811400 | 0.606548 | 0.930890 | 0.934740 | 0.930890 | 0.931249 | . 600 | 0.361000 | 0.409591 | 0.936126 | 0.937213 | 0.936126 | 0.936149 | . 800 | 0.171400 | 0.335438 | 0.927749 | 0.928861 | 0.927749 | 0.927565 | . 1000 | 0.119500 | 0.310710 | 0.935079 | 0.936808 | 0.935079 | 0.935311 | . 1200 | 0.084400 | 0.289761 | 0.937173 | 0.938265 | 0.937173 | 0.937235 | . 1400 | 0.069500 | 0.285235 | 0.938220 | 0.939314 | 0.938220 | 0.938348 | . &lt;/div&gt; &lt;/div&gt; ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0001/checkpoint-200 Configuration saved in vit-run#0001/checkpoint-200/config.json Model weights saved in vit-run#0001/checkpoint-200/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0001/checkpoint-400 Configuration saved in vit-run#0001/checkpoint-400/config.json Model weights saved in vit-run#0001/checkpoint-400/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0001/checkpoint-600 Configuration saved in vit-run#0001/checkpoint-600/config.json Model weights saved in vit-run#0001/checkpoint-600/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0001/checkpoint-800 Configuration saved in vit-run#0001/checkpoint-800/config.json Model weights saved in vit-run#0001/checkpoint-800/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0001/checkpoint-1000 Configuration saved in vit-run#0001/checkpoint-1000/config.json Model weights saved in vit-run#0001/checkpoint-1000/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0001/checkpoint-1200 Configuration saved in vit-run#0001/checkpoint-1200/config.json Model weights saved in vit-run#0001/checkpoint-1200/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0001/checkpoint-1400 Configuration saved in vit-run#0001/checkpoint-1400/config.json Model weights saved in vit-run#0001/checkpoint-1400/pytorch_model.bin Training completed. Do not forget to share your model on huggingface.co/models =) Loading best model from vit-run#0001/checkpoint-1400 (score: 0.28523513674736023). . TrainOutput(global_step=1515, training_loss=0.48840886326906313, metrics={&#39;train_runtime&#39;: 686.8561, &#39;train_samples_per_second&#39;: 35.218, &#39;train_steps_per_second&#39;: 2.206, &#39;total_flos&#39;: 1.874833643725701e+18, &#39;train_loss&#39;: 0.48840886326906313, &#39;epoch&#39;: 5.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . &#10024; Evaluation &#10024; . The metrics computed for the validation set during training looks promising. . Let&#39;s evaluate our fine-tuned ViT on the test set. . trainer.evaluate(data_cleaned[&#39;test&#39;]) . ***** Running Evaluation ***** Num examples = 952 Batch size = 16 . . [60/60 00:10] {&#39;epoch&#39;: 5.0, &#39;eval_accuracy&#39;: 0.9474789915966386, &#39;eval_f1&#39;: 0.9474429843392228, &#39;eval_loss&#39;: 0.25897082686424255, &#39;eval_precision&#39;: 0.9481391625741377, &#39;eval_recall&#39;: 0.9474789915966386, &#39;eval_runtime&#39;: 12.0318, &#39;eval_samples_per_second&#39;: 79.124, &#39;eval_steps_per_second&#39;: 4.987} . We can assume as a simple baseline the accuracy achieved by a hypothetical model that always predicts the most common class in the training set. Since in our case the most representative class is pineapple with 260 images and the train set overall contains 4,838 images, with this heuristic we would achieve an train accuracy of about 5.37%. . With the test accuracy well above 90% our fine-tuned ViT is definitely outperforming the baseline and in a real-world scenario it would justify its deployment in production. . We can get a better understanding of the results with the help of a classification report and confusion matrix both from sklearn. . Before doing that, we need to write a function to score each data point and generate the final predictions. First we leverage the predict method of the Trainer class to obtain the logits and then we use numpy.argmax to find the index corresponding to the largest logit. . That index represents the predicted class. . def get_predictions(dataset, model): raw_preds = model.predict(dataset) logits = raw_preds.predictions targets = raw_preds.label_ids preds = np.argmax(logits, axis=-1) return (targets, preds) # get indexes and target labels targets, preds = get_predictions(data_cleaned[&#39;test&#39;], trainer) . ***** Running Prediction ***** Num examples = 952 Batch size = 16 . . [60/60 02:04] from sklearn.metrics import classification_report print(classification_report(targets, preds, target_names=labels.names)) . precision recall f1-score support apple 0.98 0.96 0.97 50 banana 0.94 1.00 0.97 50 cake 0.87 0.90 0.88 50 candy 0.98 0.92 0.95 50 carrot 0.96 0.94 0.95 50 cookie 0.98 0.94 0.96 50 doughnut 0.91 1.00 0.95 50 grape 1.00 0.98 0.99 50 hot dog 0.94 0.98 0.96 50 ice cream 0.96 0.92 0.94 50 juice 0.94 0.98 0.96 50 muffin 0.86 0.92 0.89 48 orange 0.94 0.94 0.94 50 pineapple 0.91 0.97 0.94 40 popcorn 0.97 0.95 0.96 40 pretzel 0.96 0.92 0.94 25 salad 0.90 0.90 0.90 50 strawberry 0.95 0.84 0.89 49 waffle 0.96 0.92 0.94 50 watermelon 1.00 1.00 1.00 50 accuracy 0.94 952 macro avg 0.95 0.94 0.94 952 weighted avg 0.95 0.94 0.94 952 . from sklearn.metrics import ConfusionMatrixDisplay import matplotlib as mpl from matplotlib import pyplot as plt mpl.rc_file_defaults() fig, ax = plt.subplots(dpi=100) label_font = {&#39;size&#39;:&#39;18&#39;} # Adjust to fit ConfusionMatrixDisplay.from_predictions( targets, preds, display_labels=labels.names, cmap=&#39;RdPu&#39;, ax=ax ) plt.xticks(rotation=90) plt.show() . . &#10024; Data augmentation &#10024; . Previously, we&#39;ve learned how to pre-process the images with the standard feature extractor imported from Hugging Face. . In this section we will learn how to replace it with a customized pre-processing pipeline that will include additional transformations to increase the variety of images we will present to the model. . Those augmented images will make our model more robust to examples never encountered during training. . For this example we will rely on the transformations available in torchvision.transforms. . In addition to Normalize and ToTensor which were already present in the ViTFeatureExtractor class, we add to our customized pre-processing pipeline RandomResizedCrop, which replaces the standard resizing step, RandomHorizontalFlip and RandomAdjustSharpness. During training, these transformations will have a chance to crop, horizontally split images and change their sharpness. . These are just a few of the transformations available in torchvision.transforms and with a similar logic it is possible to integrate transformations from other libraries, such as albumentations. . It is the task at hand that defines which ones are worth including because with data augmentation we don&#39;t want to simply improve variety, we also want to generate plausible images that the model might have a chance to encounter once in production. For example, if we are trying to build a model to classify images of cars, probably applying a vertical flip is not the best option because in normal condition images of upside down cars are not that common. A horizontal flip might be a more suitable option. . from torchvision.transforms import ( Compose, Normalize, Resize, RandomResizedCrop, RandomHorizontalFlip, RandomAdjustSharpness, ToTensor, ToPILImage ) . When it comes to data augmentation, the dominant approach requires that random transformations are only applied to the train data, whereas the validation and test sets are subject to deterministic augmentations. In doing so, during each epoch the model has a chance to be exposed to slightly different images, but in the evaluation step the model is always scored against the same set of images. . For this example we will follow this strategy. . For resizing and normalization we need to make sure we align with the values used in the feature extractor. We can access the exact numbers through the size, image_mean and image_std attributes. . For all the random transformation, we set the probability of being applied to 0.5. . Finally, we can chain all those transformations together with Compose. . # train train_aug_transforms = Compose([ RandomResizedCrop(size=feature_extractor.size), RandomHorizontalFlip(p=0.5), RandomAdjustSharpness(sharpness_factor=5, p=0.5), ToTensor(), Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std), ]) # validation/test valid_aug_transforms = Compose([ Resize(size=(feature_extractor.size, feature_extractor.size)), ToTensor(), Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std), ]) . Let&#39;s check both pre-processing pipelines on the sample image we defined earlier. . nrows = 2 ncols = 5 plt.figure(figsize=(ncols*2, nrows*2), dpi=100) for i in range(1, nrows*ncols + 1): plt.subplot(nrows, ncols, i) plt.imshow(ToPILImage()(train_aug_transforms(sample_image))) plt.axis(&#39;off&#39;) plt.show() . nrows = 2 ncols = 5 plt.figure(figsize=(ncols*2, nrows*2), dpi=100) for i in range(1, nrows*ncols + 1): plt.subplot(nrows, ncols, i) plt.imshow(ToPILImage()(valid_aug_transforms(sample_image))) plt.axis(&#39;off&#39;) plt.show() . They both seem to work as expected: the train pipeline produced slightly different variations of the same image, whereas the validation/test pipeline generated the same output at each iteration. . Since we have two distinct transformation pipelines, we need to define two distinct functions as well. Then we pass them to each dataset independently with the set_transform method. . def apply_train_aug_transforms(examples): examples[&#39;pixel_values&#39;] = [train_aug_transforms(img.convert(&#39;RGB&#39;)) for img in examples[&#39;image&#39;]] return examples def apply_valid_aug_transforms(examples): examples[&#39;pixel_values&#39;] = [valid_aug_transforms(img.convert(&#39;RGB&#39;)) for img in examples[&#39;image&#39;]] return examples . data_preprocessed[&#39;train&#39;].set_transform(apply_train_aug_transforms) data_preprocessed[&#39;validation&#39;].set_transform(apply_valid_aug_transforms) data_preprocessed[&#39;test&#39;].set_transform(apply_valid_aug_transforms) data_preprocessed . DatasetDict({ train: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 4838 }) test: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 952 }) validation: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 955 }) }) . Just to be 100% sure everything went well, let&#39;s visualize the outputs one last time. . nrows = 2 ncols = 5 plt.figure(figsize=(ncols*2, nrows*2), dpi=100) for i in range(1, nrows*ncols + 1): plt.subplot(nrows, ncols, i) plt.imshow(ToPILImage()(data_preprocessed[&#39;train&#39;][0][&#39;pixel_values&#39;])) plt.axis(&#39;off&#39;) plt.show() . nrows = 2 ncols = 5 plt.figure(figsize=(ncols*2, nrows*2), dpi=100) for i in range(1, nrows*ncols + 1): plt.subplot(nrows, ncols, i) plt.imshow(ToPILImage()(data_preprocessed[&#39;test&#39;][0][&#39;pixel_values&#39;])) plt.axis(&#39;off&#39;) plt.show() . Everything is looking good! . data_cleaned = data_preprocessed.rename_column(&#39;label&#39;, &#39;labels&#39;) data_cleaned . DatasetDict({ train: Dataset({ features: [&#39;image&#39;, &#39;labels&#39;], num_rows: 4838 }) test: Dataset({ features: [&#39;image&#39;, &#39;labels&#39;], num_rows: 952 }) validation: Dataset({ features: [&#39;image&#39;, &#39;labels&#39;], num_rows: 955 }) }) . Training . As for the training nothing will change. . model = ViTForImageClassification.from_pretrained( checkpoint, num_labels=labels.num_classes, id2label={index: label for index, label in enumerate(labels.names)}, label2id={label: index for index, label in enumerate(labels.names)} ) . from transformers import TrainingArguments training_args = TrainingArguments( output_dir=&#39;vit-run#0002&#39;, num_train_epochs=5, per_device_train_batch_size=16, per_device_eval_batch_size=16, save_steps=200, logging_steps=200, evaluation_strategy=&#39;steps&#39;, eval_steps=200, load_best_model_at_end=True, remove_unused_columns=False, fp16=True ) . PyTorch: setting up devices The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-). . from transformers import Trainer trainer = Trainer( model, training_args, data_collator=collate_fn, train_dataset=data_cleaned[&#39;train&#39;], eval_dataset=data_cleaned[&#39;validation&#39;], compute_metrics=compute_metrics_fn ) . Using amp half precision backend . trainer.train() . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 4838 Num Epochs = 5 Instantaneous batch size per device = 16 Total train batch size (w. parallel, distributed &amp; accumulation) = 16 Gradient Accumulation steps = 1 Total optimization steps = 1515 . . [1515/1515 11:24, Epoch 5/5] Step Training Loss Validation Loss Accuracy Precision Recall F1 . 200 | 2.170600 | 1.361377 | 0.905759 | 0.909594 | 0.905759 | 0.904600 | . 400 | 1.055500 | 0.757420 | 0.919372 | 0.923756 | 0.919372 | 0.919631 | . 600 | 0.668000 | 0.522591 | 0.934031 | 0.936073 | 0.934031 | 0.934357 | . 800 | 0.482800 | 0.422806 | 0.935079 | 0.937105 | 0.935079 | 0.935202 | . 1000 | 0.414700 | 0.354634 | 0.938220 | 0.939484 | 0.938220 | 0.938099 | . 1200 | 0.353700 | 0.316942 | 0.946597 | 0.947634 | 0.946597 | 0.946709 | . 1400 | 0.320300 | 0.305057 | 0.949738 | 0.950471 | 0.949738 | 0.949751 | . &lt;/div&gt; &lt;/div&gt; ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0002/checkpoint-200 Configuration saved in vit-run#0002/checkpoint-200/config.json Model weights saved in vit-run#0002/checkpoint-200/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0002/checkpoint-400 Configuration saved in vit-run#0002/checkpoint-400/config.json Model weights saved in vit-run#0002/checkpoint-400/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0002/checkpoint-600 Configuration saved in vit-run#0002/checkpoint-600/config.json Model weights saved in vit-run#0002/checkpoint-600/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0002/checkpoint-800 Configuration saved in vit-run#0002/checkpoint-800/config.json Model weights saved in vit-run#0002/checkpoint-800/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0002/checkpoint-1000 Configuration saved in vit-run#0002/checkpoint-1000/config.json Model weights saved in vit-run#0002/checkpoint-1000/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0002/checkpoint-1200 Configuration saved in vit-run#0002/checkpoint-1200/config.json Model weights saved in vit-run#0002/checkpoint-1200/pytorch_model.bin ***** Running Evaluation ***** Num examples = 955 Batch size = 16 Saving model checkpoint to vit-run#0002/checkpoint-1400 Configuration saved in vit-run#0002/checkpoint-1400/config.json Model weights saved in vit-run#0002/checkpoint-1400/pytorch_model.bin Training completed. Do not forget to share your model on huggingface.co/models =) Loading best model from vit-run#0002/checkpoint-1400 (score: 0.3050570487976074). . TrainOutput(global_step=1515, training_loss=0.7456918596827945, metrics={&#39;train_runtime&#39;: 685.1737, &#39;train_samples_per_second&#39;: 35.305, &#39;train_steps_per_second&#39;: 2.211, &#39;total_flos&#39;: 1.874833643725701e+18, &#39;train_loss&#39;: 0.7456918596827945, &#39;epoch&#39;: 5.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Evaluation . Let&#39;s see how the model performs on the test set. . trainer.evaluate(data_cleaned[&#39;test&#39;]) . ***** Running Evaluation ***** Num examples = 952 Batch size = 16 . . [60/60 00:33] {&#39;eval_accuracy&#39;: 0.944327731092437, &#39;eval_f1&#39;: 0.9442795351899795, &#39;eval_loss&#39;: 0.3101879954338074, &#39;eval_precision&#39;: 0.945520405649697, &#39;eval_recall&#39;: 0.944327731092437, &#39;eval_runtime&#39;: 12.514, &#39;eval_samples_per_second&#39;: 76.075, &#39;eval_steps_per_second&#39;: 4.795} . Even though this second model achieves a slightly lower test accuracy, it is probably more robust than the previous one thanks to the augmented images. . It&#39;s also important to notice that at the end of this second run the training loss is still larger than the validation loss. This might be a sign that the model is underfitting the training set and there might still be room for improvement by training for additional epochs. . targets, preds = get_predictions(data_cleaned[&#39;test&#39;], trainer) . ***** Running Prediction ***** Num examples = 952 Batch size = 16 . . [60/60 00:52] from sklearn.metrics import classification_report print(classification_report(targets, preds, target_names=labels.names)) . precision recall f1-score support apple 0.98 0.96 0.97 50 banana 0.94 1.00 0.97 50 cake 0.87 0.90 0.88 50 candy 0.98 0.92 0.95 50 carrot 0.96 0.94 0.95 50 cookie 0.98 0.94 0.96 50 doughnut 0.91 1.00 0.95 50 grape 1.00 0.98 0.99 50 hot dog 0.94 0.98 0.96 50 ice cream 0.96 0.92 0.94 50 juice 0.94 0.98 0.96 50 muffin 0.86 0.92 0.89 48 orange 0.94 0.94 0.94 50 pineapple 0.91 0.97 0.94 40 popcorn 0.97 0.95 0.96 40 pretzel 0.96 0.92 0.94 25 salad 0.90 0.90 0.90 50 strawberry 0.95 0.84 0.89 49 waffle 0.96 0.92 0.94 50 watermelon 1.00 1.00 1.00 50 accuracy 0.94 952 macro avg 0.95 0.94 0.94 952 weighted avg 0.95 0.94 0.94 952 . from sklearn.metrics import ConfusionMatrixDisplay import matplotlib as mpl from matplotlib import pyplot as plt mpl.rc_file_defaults() fig, ax = plt.subplots(dpi=100) label_font = {&#39;size&#39;:&#39;18&#39;} # Adjust to fit ConfusionMatrixDisplay.from_predictions( targets, preds, display_labels=labels.names, cmap=&#39;RdPu&#39;, ax=ax ) label_font = {&#39;size&#39;:&#39;10&#39;} # Adjust to fit ax.set_xlabel(&#39;Predicted labels&#39;, fontdict=label_font) ax.set_ylabel(&#39;Observed labels&#39;, fontdict=label_font) ax.tick_params(axis = &#39;both&#39;, labelsize=7) plt.xticks(rotation=90) plt.show() . . &#10024; Conclusion &#10024; . In this post we&#39;ve learned that the ViT is an encoder-based architecture designed to perform computer vision tasks. One of the most interesting aspects is how images are pre-processed and fed to the model: each image is divided into 16x16 patches and those patches are transformed into a sequence of embeddings to form the input data. . Then we&#39;ve discovered how to take a pre-trained model from Hugging Face and fine-tune it to perform image classification on a task-specific dataset. . Finally we&#39;ve learned how to enhance the default pre-processing pipeline with additional data augmentation transformations to fine-tune a more performing and robust model. . . &#129303; Acknowledgments &#129303; . A big shout out to the Hugging Face community. In particular, I would like to personally thank Niels Rogge and Nate Raw for their contributions to the broader community. Niels&#39; notebook and Nate&#39;s post were fundamental sources of inspiration to write this blog post. A special thanks also to Matthijs Hollemans for making available the snacks dataset on the Hub. | Sergios Karagiannakos&#39; ViT tutorial was another great source of knowledge and inspiration. | . . &#128218; Additional resources &#128218; . The ViT paper and Google AI&#39;s blog post are two great resources to explore more in depth the theory behind the ViT. | If you prefer videos over articles, then Yannic Kilcher&#39;s ViT paper explanation might be the right resource for you to learn more about the ViT architecture. | The Hugging Face Course and Docs are the go-to resources to clarify doubts and learn more about the Hugging Face libraries in general. | . &lt;/div&gt; .",
            "url": "https://matteopilotto.github.io/blog/2022/05/21/vit_snacks_finetune.html",
            "relUrl": "/2022/05/21/vit_snacks_finetune.html",
            "date": " ‚Ä¢ May 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "How to finetune a transformer model with Hugging Face ü§ó",
            "content": ". Part 1 - Data pre-processing and fine-tuning . &#128278; . In A quick review of natural language processing we will introduce transformer-based models and how they can help us to deal with natural language tasks. | In Loading the data we will learn how to load a dataset from the Hugging Face Hub. | In Taking a bird‚Äôs-eye view of the ML pipeline for NLP we will learn about a common pipeline to fine tune language models. | In Pre-processing phase we will learn how to prepare a dataset for training a language model. | In Training phase we will discover the necessary steps to finetune a transformer model to perform a down-stream task. | In Post-processing/Evaluation phase we will see how to evaluate a transformer-based model. | In Improving the pipeline we will explore different ways to enhance the training loop. | Additional resources contains extra materials to delve into transformers. | Acknowledgement | . &#10024; A quick review of natural language processing (NLP) &#10024; . Natural language processing (NLP) is the branch of AI that studies how computers can process human language in the form of text or voice and make sense of those words. . NLP techniques can be used to solve a large variety of tasks such as: . sentiment analysis. . | machine translation. . | natural language generation. . | . NLP is notoriously known as one of the most challenging field in AI because human languages are ever-changing systems defined by complex sets of grammar rules. Languages also come with a variety of exceptions, ambiguities and irregularities that sometimes are not even explicitly formalized in written rules. For these reasons traditional approaches like rule-based programs have always struggled to perform tasks involving human languages. . This is where machine learning and specifically transformer models can help us. . &#10024; Transformers &#10024; . &#11088; A bit of history &#11088; . The transformer architecture was proposed in the seminal paper Attention is all you need by Ashish Vaswani et al. in 2017. The original transformer model was specifically designed to perform machine translation (e.g. translating text from english to french and vice versa), but thanks to its effectiveness and flexibility, in the following years several other architectures have been developed based on this original idea. These transformer-based models represent the state-of-the-art models in most NLP tasks (e.g. summarization and question answering) and recently they have also been successfully employed to tackle computer vision (CV) tasks such as image classification and segmentation. . Over the years, the number of transformer-based models has grown exponentially and it would be pointless trying to learn all of them. However, there are four foundational models that any NLP practitioners should be aware of: . GPT was published by OpenAI in June 2018 and it represented the first pre-trained transformer model. It employs only the decoder part. . | BERT is another pre-trained model published by Google in October 2018. It uses only the encoder part. . | BART and T5 were published in October 2019 by Facebook and Google, respectively. They leverage both the encoder and decoder as the original transformer. . | . &#11088; What makes the transformer models work so well &#11088; . Fundamentally, there are two key features that make the transformer architecture so successful compared to other solutions: . transformer models can process sequential data (e.g. words in a sentence) all at once, taking full advantage of the parallelization capabilities offered by GPUs. . | Transformer models incorporate attention layers. These special layers allow the model to pay attention to other sections of the input data when processing a specific point in the sequence. This aspect is extremely relevant in NLP where the meaning of words is heavily affected by the context. . | &#11088; The key components of the original transformer architecture &#11088; . Essentially, the original transformer model is made of two pieces: the encoder and the decoder. . The encoder is a stack of smaller components, called encoder blocks, and each block can be further broken down into two sublayers: . a bidirectional self-attention layer. . | a feed-forward neural network. . | . Similarly, the decoder is a stack of decoder blocks, but the internal sublayers are slightly different: . a masked self-attention layer. . | an encoder-decoder attention layer. . | a feed-forward neural network. . | . . &#11088; Three families of transformers &#11088; . One interesting aspect of transformer models is their modular architecture: a transformer does not always require an encoder and a decoder to work properly. For this reason, they can be grouped in three main families based on the components they incorporate: . GPT-like or auto-regressive models. These models use only the decoder part and they are well suited for generation tasks such as text and image generation. . | BERT-like or auto-encoding models. These models leverage only the encoder part and they work well for natural language understanding tasks such as text-classification, named entity recognition (i.e. word classification) and extractive question answering. . | BART/T5-like or sequence-to-sequence models. These models make use of both the encoder and the decoder. They are designed to perform generative task subject to a certain input such as translation and summarization. . | . Now that we have a basic understanding of transformers models and their components, let‚Äôs see how we can leverage one of these pre-trained models to perform sentiment analysis. Along the way, we will explore new concepts and we will dig deeper into some of the ideas we came across in this section. . . &#10024; Fine-tuning a transformer model to recognize emotions &#128514;&#128546;&#128545;&#128515;&#128559; &#10024; . We will fine-tune a transformer model to perform sentiment analysis on tweets. . From the ML perspective, sentiment analysis is just a sentence-classification task. We want to assign the right label to a sentence. . To do that we will use the Hugging Face library which is by far the most popular resource used by practitioners to perform NLP tasks with transformers. On Hugging Face we have access to all sorts of things, from pre-trained models to pre-processed datasets. All of the code is open-source and the API is extremely intuitive. . &#11088; Setup &#11088; . Transformer-based models are fairly large and even though we don&#39;t need to train one from scratch, we still want to take advantage of GPUs. Let&#39;s make sure we have access to one. . !nvidia-smi . Sat Apr 23 23:05:47 2022 +--+ | NVIDIA-SMI 460.32.03 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 42C P0 28W / 250W | 0MiB / 16280MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . Now that we&#39;re sure we have access to a GPU, we can install the Hugging Face libraries: . datasets gives us direct access to the datasets available on Hugging Face. . | transformers[sentencepiece] contains the model architectures and the pre-trained parameters. . | . !pip install datasets -Uqq !pip install transformers[sentencepiece] -Uqq . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 325 kB 8.0 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 45.5 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77 kB 5.5 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212 kB 51.5 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136 kB 54.4 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127 kB 55.2 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144 kB 53.1 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271 kB 49.0 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94 kB 3.1 MB/s ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.0 MB 7.3 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 50.9 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 45.1 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 40.9 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 37.1 MB/s . This piece of code is just to make sure we all get the same numbers... . import numpy as np import random import torch def set_seeds(seed=1234): &quot;&quot;&quot;Set seeds for reproducibility.&quot;&quot;&quot; np.random.seed(seed) random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) set_seeds(2077) . . &#11088; Loading the data &#11088; . In this example we won&#39;t simply classify tweets as positive or negative, but we will go a step forward, trying to classify them based on the underlying emotion. Thankfully we don&#39;t have to spend time to collect and create our own data because on Hugging Face we can easily access the emotion dataset. From the documentation on the Hugging Face website, we can get some basic information regarding the dataset. It contains English tweets labeled based on six emotions: joy, love, surprise, sadness, anger and fear. . The data is also conveniently divided in a train, validation and test set. . Loading the data in our environment is easy. We just need to import the load_dataset function from the datasets library and pass the name of the data we want as the first parameter. . from datasets import load_dataset dataset = load_dataset(&#39;emotion&#39;) . Using custom data configuration default . Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705... Dataset emotion downloaded and prepared to /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data. . The data is store inside a DatasetDict object which is essentially a dictionary with additional functionalities. Each key-value pair corresponds to a dataset split. In our case we have three splits (train, validation and test) defined as a Dataset object which is another form of dictionariy. . dataset . DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 16000 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 2000 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;], num_rows: 2000 }) }) . One handy feature of Dataset objects is that we can explore them as if there were lists. By examining the train split we can see that label contains our target classes (i.e. our six emotions) encoded from 0 to 5 and text contains the tweets that we will feed to the transformer model as input features. . dataset[&#39;train&#39;][:3] . {&#39;label&#39;: [0, 0, 3], &#39;text&#39;: [&#39;i didnt feel humiliated&#39;, &#39;i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake&#39;, &#39;im grabbing a minute to post i feel greedy wrong&#39;]} . dataset[&#39;train&#39;][&#39;text&#39;][:3] . [&#39;i didnt feel humiliated&#39;, &#39;i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake&#39;, &#39;im grabbing a minute to post i feel greedy wrong&#39;] . To figure out which target ID corresponds to which emotion we can access the features attribute to take a look at the datatypes. Our target variable is a ClassLabel object and its names attribute contains the names of the emotions in the correct order. . dataset[&#39;train&#39;].features . {&#39;label&#39;: ClassLabel(num_classes=6, names=[&#39;sadness&#39;, &#39;joy&#39;, &#39;love&#39;, &#39;anger&#39;, &#39;fear&#39;, &#39;surprise&#39;], id=None), &#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)} . dataset[&#39;train&#39;].features[&#39;label&#39;].names . [&#39;sadness&#39;, &#39;joy&#39;, &#39;love&#39;, &#39;anger&#39;, &#39;fear&#39;, &#39;surprise&#39;] . Let&#39;s save both the names and numbers of labels. We can use the label names to create a dictionary mapping the IDs to labels and vice versa. Instead, we will use the number of labels in a few moments to properly define the shape of the output vector of our transformer model. . label_names = dataset[&#39;train&#39;].features[&#39;label&#39;].names num_labels = dataset[&#39;train&#39;].features[&#39;label&#39;].num_classes . ids2labels = {i: label for i, label in enumerate(label_names)} ids2labels . {0: &#39;sadness&#39;, 1: &#39;joy&#39;, 2: &#39;love&#39;, 3: &#39;anger&#39;, 4: &#39;fear&#39;, 5: &#39;surprise&#39;} . Here we are just scratching the surface. Hugging Face DatasetDict and Dataset are extremely powerful classes that offer many other functionalities. . They are also well suited to perform exploratory data analysis (EDA). For the sake of simplicity, we won&#39;t perform any EDA, but when working on a real project, it is a step that we should definitely go through. To learn more about the DatasetDict and Dataset check out the official documentation. . Also keep in mind that there is no standard way to explore datasets. Each dataset has its own peculiarities and this affects how we interact and what we can do with them. That&#39;s why I encourage you, once you have gone through this notebook, to dedicate some time to explore other datasets. Here you can find all the datasets currently available on Hugging Face. The different filters on the left-hand side makes the searching process straightforward. . . &#11088; Taking a bird&#8217;s-eye &#129413; view of the ML pipeline for NLP &#11088; . Now that we have the data, we are ready to move to the actual ML pipeline for NLP tasks. Again, there is no silver bullet, but usually it consists of three steps: . pre-processing phase: unfortunately we cannot feed raw text directly to transformer models. This is the reason why the first step is converting raw text into numbers. We will do that by using a tokenizer. . | training phase: this is where we feed the pre-processed data to the model in order to update the parameters. We will use a pre-trained BERT to build our sentence-classification model. . | post-processing/evaluation phase: in this step we take the model outputs and convert them into a format we can interpret. For our task, we will convert the transformer outputs into probabilities. . | . &#11088; Pre-processing phase &#11088; . Tokenizers are the perfect tools to pre-process raw text. They take a list of sentences as input and return a list of integers plus any other optional output. Tokens are the atomic components of a sentence and depending on the tokenization strategy used they can be words, sub-words or even single characters. A vocabulary (or vocab) is a dictionary mapping tokens to integers and vice versa. The integers in the output list are often called token IDs. . In general the tokenization process involves three steps: . the tokenizer receives the input text and breaks it down into tokens. . | it assigns each token to an integer. . | depending on the transformer model considered, the tokenizer adds special tokens to the vocab. . | ‚ÄºÔ∏è Different transformer models require different special tokens. That&#39;s why it&#39;s paramount to always make sure that the transformer and the tokenizer use the same vocabulary. ‚ÄºÔ∏è . In practice, the first thing we need to do is to load a tokenizer from the Hugging Face library. AutoTokenizer is the generic tokenizer class and it can be instantiated based on a pre-trained model vocabulary using the from_pretrained method and passing the name of the model as argument. As we have just mentioned, we will use a pre-trained BERT model called bert-base-uncased because generally BERT-like models perform very well in sentence-classification tasks. . In most cases, the Hugging Face website provides useful information regarding the pre-trained models. Here we can find the page for our BERT model, while here we can find the full list of pre-trained models available. In the long run, it always pays off to dedicate some time reading the model description in order to familiarize with them. . After defining our AutoTokenizer object, we can call it to get additional information: . vocab_size represents the number of tokens a tokenizer can distinguish. Ours can recognize 30,522 unique tokens. . | model_max_len is an argument related to our transformer model and tells us the maximum length of the input sequences we can feed to the model. The BERT model we will use can accept inputs up to 512 tokens long. . | padding_side and truncation_side are two important concepts that define how the tokenizer will handle inputs of different length when they are processed in batches: . Padding is the process of adding special tokens, called padding tokens, to sequences with fewer tokens. . | Truncation is the process of reducing the length of the sequences that are too long. . | . To learn more about these two concepts check out the documentation. . | special_tokens, sep_token, pad_token, cls_token and mask_token describe all the extra tokens automatically added by the tokenizer when processing a sentence. These tokens guide the model to properly processing the input data. . | . from transformers import AutoTokenizer checkpoint = &#39;bert-base-uncased&#39; tokenizer = AutoTokenizer.from_pretrained(checkpoint) tokenizer . PreTrainedTokenizerFast(name_or_path=&#39;bert-base-uncased&#39;, vocab_size=30522, model_max_len=512, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens={&#39;unk_token&#39;: &#39;[UNK]&#39;, &#39;sep_token&#39;: &#39;[SEP]&#39;, &#39;pad_token&#39;: &#39;[PAD]&#39;, &#39;cls_token&#39;: &#39;[CLS]&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;}) . Now that we have our tokenizer, let&#39;s see how it works on a few sample sentences. To do that we just need to pass a list of sentences to the tokenizer and it will automatically perform the three steps we mentioned earlier. What we get as output is a dictionary containing three key-value pairs: . input_ids contains the token IDs. They are the main input data of the transformer model. . | attention_mask defines which other parts of the input sentence the attention layers of the transformer model can attend to when processing a certain position. It&#39;s a list of booleans where 1 means the attention layers can pay attention to that token. There are two common situations where we want the attention layers to ignore certain token IDs: . We are performing a generative task, such as text generation, and we don&#39;t want our model to peek at the tokens it hasn&#39;t generated yet. It would be like cheating! . | We are padding the input text and, since padding tokens don‚Äôt have any semantic meaning, we don‚Äôt want our model to make predictions based on those tokens. . | . In our case we get only 1s because we are not trying to generate text and, at least for now, we haven&#39;t padded our sentences. . | Our BERT model can potentially digest a pair of sentences. We can find this information in the model description. token_type_ids tells the model if a certain token belongs to the first or second sentence. 0 indicates the first sentence and 1 the second one. Since in our example we are not dealing with pairs, we just get lists of 0s. . | . sample_text = dataset[&#39;train&#39;][&#39;text&#39;][:3] sample_text . [&#39;i didnt feel humiliated&#39;, &#39;i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake&#39;, &#39;im grabbing a minute to post i feel greedy wrong&#39;] . sample_output = tokenizer(sample_text) sample_output . {&#39;input_ids&#39;: [[101, 1045, 2134, 2102, 2514, 26608, 102], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102], [101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} . Using the convert_ids_to_tokens method we can convert a list of IDs back to tokens. This gives us a chance to get a better understanding of how the tokenizer works. The output list provides two interesting insights: . our tokenizer uses a sub-word tokenization strategy because we can see that the word didnt has been split into didn and ##t. . | it automatically takes care of special tokens. The pre-processing description informs us that the special token [CLS] represents the beginning of a sentence , whereas the [SEP] token is used to separate two paired sentences or signal the end of the input sentence. . | tokenizer.convert_ids_to_tokens(sample_output[&#39;input_ids&#39;][0]) . [&#39;[CLS]&#39;, &#39;i&#39;, &#39;didn&#39;, &#39;##t&#39;, &#39;feel&#39;, &#39;humiliated&#39;, &#39;[SEP]&#39;] . We can also get back the original input data by using the decode method. We need to set the argument skip_special_tokens=True to tell the tokenizer to drop any special tokens. . tokenizer.decode(sample_output[&#39;input_ids&#39;][0], skip_special_tokens=True) . &#39;i didnt feel humiliated&#39; . We can easily pad and truncate our sentences by setting truncation=True and padding=True. To learn more about the different strategies available check the documentation. . Now that we have familiarized ourselves with the tokenizer, we are ready to apply it to our three datasets. The most intuitive way is to pass the text data of each split to the tokenizer. More specifically, this approach can be broken down into 5 steps: . we create an empty DatasetDict object to collect both the raw and tokenized data. . | we pass the sentences of each split to the tokenizer. As we have just mentioned, we set truncation=True and padding=True to make sure the tokenized sequences have the same length. . | we convert the output dictionary into a Dataset object. . | we use the concatenate_datasets function from the datasets library to concatenate the raw and tokenized data together. This step is totally optional. There might be cases where we don&#39;t need to retain the original data. . | we repeat step 2, 3 and 4 for all the remaining splits in our DatasetDict. . | The final output of this process is a DatasetDict object containing for each split the raw and tokenized data. . # ref: https://discuss.huggingface.co/t/add-new-column-to-a-dataset/12149 from datasets import DatasetDict, Dataset, concatenate_datasets # instantiace a empty DatasetDict object tokenized_datasets = DatasetDict() for split in dataset.keys(): # apply tokenizer to dataset split. It return a dictionary tokenized_split = tokenizer(dataset[split][&#39;text&#39;], truncation=True, padding=True) # convert dictionary into a Dataset object tokenized_split = Dataset.from_dict(tokenized_split) # add row data and tokenizer outputs to the DatasetDict object tokenized_datasets[split] = concatenate_datasets([dataset[split], tokenized_split], axis=1) . tokenized_datasets . DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 16000 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) }) . Let&#39;s double check that our tokenized sentences have the same length. . samples = tokenized_datasets[&#39;train&#39;][:10] samples = {k: v for k, v in samples.items()} [len(x) for x in samples[&#39;input_ids&#39;]] . [87, 87, 87, 87, 87, 87, 87, 87, 87, 87] . The process just described is intuitive and it works perfectly fine in most situations. However, it doesn&#39;t scale well with very large datasets. That&#39;s why we will explore an alternative approach which at the beginning might feel more convoluted, but in the long run will provide more flexibility and capabilities. This second approach revolves around two key concepts: mapping and dynamic padding . The DatasetDict class has a map method that enables us to apply a function to each dataset in the object. This function takes a dictionary as input and returns a dictionary as output. In our specific case, the function will apply the tokenizer to our three datasets. The map method is designed to automatically add the key-value pairs of the output dictionary created by the tokenizer to the pre-existing dictionary. That&#39;s why we no longer need to concatenate the data. To get more details about the map method check the documentation. . ‚ÄºÔ∏è Keep in mind that if a key already exists, the new values will replace the existing ones. ‚ÄºÔ∏è . Dynamic padding is a separate step after the tokenization and in order to work properly it requires non padded data. That&#39;s why in the tokenization step we only apply truncation. . In addition, we set batched=True to provide a batch of data to the function to speed up the process. . def get_tokens(split): return tokenizer(split[&#39;text&#39;], truncation=True) # lambda version # tokenized_datasets = dataset.map(lambda split: tokenizer(split[&#39;text&#39;], truncation=True), batched=True) tokenized_datasets = dataset.map(get_tokens, batched=True) tokenized_datasets . DatasetDict({ train: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 16000 }) validation: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) test: Dataset({ features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) }) . By default when we set padding=True the tokenizer pads all sentences to match the longest sentence in the entire dataset. Conversely, with dynamic padding we pad our data dynamically when forming the batches based on the longest sequence in each batch. As a result, the padded sequences in a specific batch have the same length, but most likely they end up having a different one when compared to padded sequences in another batch. In most cases dynamic padding speeds up training compared to normal padding because we avoid passing to the model over-long sequences containing uninfortmative pad tokens. . To apply dynamic padding we just need to import the class DataCollatorWithPadding from the transformers library and pass our tokenzier as argument. During training, the data collator will be responsible to create batches and dynamically pad the input sequences accordingly. . Let&#39;s see how it works on a sample. . As we can see, since we didn&#39;t apply padding, our sequences in the sample have varying lengths after the tokenization. The next step consists in passing the sample to the DataCollatorWithPadding object to pad the input data based on the longest sequence in the sample, which in this case is 30 tokens. As expected, if we try to apply the data collator on a different sample we get a different maximum length (44 tokens). . samples = tokenized_datasets[&#39;train&#39;][:10] samples = {k: v for k, v in samples.items() if k not in [&#39;text&#39;, &#39;label&#39;]} [len(x) for x in samples[&#39;input_ids&#39;]] . [7, 23, 12, 22, 8, 17, 30, 20, 25, 6] . from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding(tokenizer=tokenizer) . batch = data_collator(samples) {k: v.shape for k, v in batch.items()} . {&#39;attention_mask&#39;: torch.Size([10, 30]), &#39;input_ids&#39;: torch.Size([10, 30]), &#39;token_type_ids&#39;: torch.Size([10, 30])} . samples = tokenized_datasets[&#39;train&#39;][10:20] samples = {k: v for k, v in samples.items() if k not in [&#39;text&#39;, &#39;label&#39;]} print([len(x) for x in samples[&#39;input_ids&#39;]], &#39; n&#39;) batch = data_collator(samples) {k: v.shape for k, v in batch.items()} . [16, 23, 14, 10, 44, 12, 9, 10, 25, 19] . {&#39;attention_mask&#39;: torch.Size([10, 44]), &#39;input_ids&#39;: torch.Size([10, 44]), &#39;token_type_ids&#39;: torch.Size([10, 44])} . We have finally come to the end of the pre-processing phase and we are ready to move to the training phase. . . &#11088; Training phase &#11088; . The training phase with the Hugging Face library can be deconstructed into four steps:1. define the model. . specify the training arguments. . | define the training loop. . | Fine-tune (or train for scratch) the model. . | Since our objective is to classify sentences based on their semantic meaning we need to import the AutoModelForSequenceClassification from the transformers library specifying the checkpoint and the number of output classes. The BERT checkpoint we‚Äôre trying to use was pre-trained to perform a fill-mask and a next-sentence-prediction task. When we import AutoModelForSequenceClassification we are telling the library we want to perform sequence-classification and, as a consequence, it automatically replaces the original model head with a new one specifically designed for our down-stream task. The term head refers to the last layer (or the last few layers) in the model architecture and is responsible to project the transformer outputs onto the proper vector space. In other words, we can adapt a pre-trained transformer model to perform any task by simply changing its head and leaving all the rest (i.e. the backbone) as it is. However, when we change head its parameters are initialized with random values. That‚Äôs why we get a warning suggesting us to train the model to be able to use it for predictions and inference. For this specific BERT model we can actually take a look at the new head using the classifier attribute. As we can see, it&#39;s just a linear layer mapping the 768-dimensional vectors generated by the last hidden layer to a 6-dimensional vector space. . The transformers library provides classes to perform all sorts of down-stream tasks. You can find them in the documentation. . from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) . Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . model.classifier . Linear(in_features=768, out_features=6, bias=True) . The next step is to define the training arguments. We can do this by importing the TrainingArguments from the transformers library. The only required parameter is the output_dir which specifies the output directory for the model predictions and checkpoints. In our case we also provide three optional parameters: . num_train_epochs defines the number of training epochs. . | evaluation_strategy tells the training script to evaluate the model at the end of each epoch. . | fp16 gives us the option to use 16-bit (mixed) precision instead of 32-bit to speed up training. . | . Check out the documentation to take a look at the full list of optional parameters. . from transformers import TrainingArguments training_args = TrainingArguments( output_dir=&#39;trainer-run#0001&#39;, num_train_epochs=1, evaluation_strategy=&#39;epoch&#39;, fp16=True ) . Actually before we can move to the third step, there is one last bit of pre-processing we need to do: . remove all the unnecessary columns in features. In our case the only column we need to remove is text. . | rename the column containing the ground truth classes labels because the training script by default will look for that column to compute the loss. This aspect is mentioned in the documentation. . | set the format for every dataset. We set the torch format because we are using Pytorch. . | . tokenized_datasets = tokenized_datasets.remove_columns(&#39;text&#39;) tokenized_datasets = tokenized_datasets.rename_column(&#39;label&#39;, &#39;labels&#39;) tokenized_datasets.set_format(&#39;torch&#39;) tokenized_datasets . DatasetDict({ train: Dataset({ features: [&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 16000 }) validation: Dataset({ features: [&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) test: Dataset({ features: [&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2000 }) }) . To define our training loop we simply need to import the Trainer class from the transformers library. This class contains a basic training loop which supports all the items we have generated so far: . pre-trained model . | training arguments . | pre-processed data . | data collator . | tokenizer . | . As always we can learn more about this class by checking out the official documentation. . from transformers import Trainer trainer = Trainer( model, training_args, data_collator=data_collator, train_dataset=tokenized_datasets[&#39;train&#39;], eval_dataset=tokenized_datasets[&#39;validation&#39;], tokenizer=tokenizer ) . Using amp half precision backend . Now that all the pieces are in place, we can finally finetune our transformer model by calling the train method. By default the training loop reports back only the training and the validation loss. Unfortunately, from a human perspective those values are not very informative. Instead, we would like to see metrics like accuracy, precision and recall. We will see how we can add those metrics to the training loop in just a moment, but first let&#39;s briefly jump to the evaluation phase where we can see how to evaluate our model and make predictions. . trainer.train() . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 16000 Num Epochs = 1 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 2000 . . [2000/2000 03:44, Epoch 1/1] Epoch Training Loss Validation Loss . 1 | 0.212300 | 0.177955 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to trainer-run#0001/checkpoint-500 Configuration saved in trainer-run#0001/checkpoint-500/config.json Model weights saved in trainer-run#0001/checkpoint-500/pytorch_model.bin tokenizer config file saved in trainer-run#0001/checkpoint-500/tokenizer_config.json Special tokens file saved in trainer-run#0001/checkpoint-500/special_tokens_map.json Saving model checkpoint to trainer-run#0001/checkpoint-1000 Configuration saved in trainer-run#0001/checkpoint-1000/config.json Model weights saved in trainer-run#0001/checkpoint-1000/pytorch_model.bin tokenizer config file saved in trainer-run#0001/checkpoint-1000/tokenizer_config.json Special tokens file saved in trainer-run#0001/checkpoint-1000/special_tokens_map.json Saving model checkpoint to trainer-run#0001/checkpoint-1500 Configuration saved in trainer-run#0001/checkpoint-1500/config.json Model weights saved in trainer-run#0001/checkpoint-1500/pytorch_model.bin tokenizer config file saved in trainer-run#0001/checkpoint-1500/tokenizer_config.json Special tokens file saved in trainer-run#0001/checkpoint-1500/special_tokens_map.json Saving model checkpoint to trainer-run#0001/checkpoint-2000 Configuration saved in trainer-run#0001/checkpoint-2000/config.json Model weights saved in trainer-run#0001/checkpoint-2000/pytorch_model.bin tokenizer config file saved in trainer-run#0001/checkpoint-2000/tokenizer_config.json Special tokens file saved in trainer-run#0001/checkpoint-2000/special_tokens_map.json ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=2000, training_loss=0.38520724105834964, metrics={&#39;train_runtime&#39;: 225.2836, &#39;train_samples_per_second&#39;: 71.022, &#39;train_steps_per_second&#39;: 8.878, &#39;total_flos&#39;: 339429562491168.0, &#39;train_loss&#39;: 0.38520724105834964, &#39;epoch&#39;: 1.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . &#11088; Post-processing/Evaluation phase &#11088; . To evaluate a model is simple. We just need to call the evaluate method and pass a dataset as a parameter. . trainer.evaluate(tokenized_datasets[&#39;test&#39;]) . ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 . . [250/250 00:05] {&#39;epoch&#39;: 1.0, &#39;eval_loss&#39;: 0.19188867509365082, &#39;eval_runtime&#39;: 5.6257, &#39;eval_samples_per_second&#39;: 355.512, &#39;eval_steps_per_second&#39;: 44.439} . Generating the predictions is slightly more convolutated because we need to apply a bit of post-processing. . In fact, after calling the predict method we get a PredictionOutput object containing three items: . predictions consists of a matrix with a row for each input sequence and a column for each label. In our case, the test set has 2000 rows and 6 columns, therefore we get a 2000 x 6 matrix. Each row vector contains logits which represent the unnormalized output scores of the transformer model. We can interpret these scores as probabilities by passing them through a softmax function. The actual prediction is the class corresponding to the index with the largest logit (or probability). . | label_ids contains the ground truth labels. . | metrics contains the metrics. For the moment we have only the loss. . | . predictions = trainer.predict(tokenized_datasets[&#39;test&#39;]) predictions . ***** Running Prediction ***** Num examples = 2000 Batch size = 8 . . [250/250 00:21] PredictionOutput(predictions=array([[ 7.215 , -1.412 , -1.734 , -0.984 , -1.61 , -1.484 ], [ 7.285 , -1.78 , -1.742 , -1.078 , -1.317 , -1.413 ], [ 7.223 , -1.898 , -1.807 , -1.092 , -1.128 , -1.452 ], ..., [-2.326 , 6.58 , -0.3184 , -1.887 , -1.687 , -1.321 ], [-2.297 , 6.38 , -0.06232, -2.316 , -1.183 , -1.547 ], [-2.316 , -0.9844 , -1.435 , -2.07 , 4.11 , 3.473 ]], dtype=float16), label_ids=array([0, 0, 0, ..., 1, 1, 4]), metrics={&#39;test_loss&#39;: 0.19188867509365082, &#39;test_runtime&#39;: 10.162, &#39;test_samples_per_second&#39;: 196.811, &#39;test_steps_per_second&#39;: 24.601}) . predictions.predictions.shape, predictions.label_ids.shape . ((2000, 6), (2000,)) . In practice, to turn logits into probabilities we import the softmax function from torch.nn.functional and pass the logit scores as torch.Tensor. We need to set dim=-1 to make sure the softmax function is applied across columns and not rows. To get the index of the largest logit/probability (i.e. the actual prediction) we can use the argmax function available in numpy. Again, we need to specify axis=-1 to apply the function across columns. . import numpy as np import torch from torch.nn.functional import softmax logits = predictions.predictions probs = softmax(torch.Tensor(logits), dim=-1) preds = np.argmax(logits, axis=-1) . probs . tensor([[9.9910e-01, 1.7905e-04, 1.2972e-04, 2.7476e-04, 1.4685e-04, 1.6657e-04], [9.9918e-01, 1.1550e-04, 1.1998e-04, 2.3309e-04, 1.8349e-04, 1.6674e-04], [9.9912e-01, 1.0924e-04, 1.1974e-04, 2.4474e-04, 2.3605e-04, 1.7069e-04], ..., [1.3553e-04, 9.9802e-01, 1.0093e-03, 2.1033e-04, 2.5695e-04, 3.7023e-04], [1.7019e-04, 9.9719e-01, 1.5900e-03, 1.6690e-04, 5.1862e-04, 3.6029e-04], [1.0496e-03, 3.9766e-03, 2.5351e-03, 1.3424e-03, 6.4819e-01, 3.4291e-01]]) . preds . array([0, 0, 0, ..., 1, 1, 4]) . This is the end of this section. . In the next one we will explore few different way to improve the current pipeline, including how to: . pass more meaningful metrics to the training loop. . | evaluate the model through more elegant visualizations. . | save the model checkpoint locally or on a cloud storage. . | . . &#11088; Improving the pipeline &#11088; . &#129680; Adding the metrics &#129680; . Now that we know how to evaluate our model and generate predictions, a nice addition would be the possibility to look at more meaningful metrics. To do that first we need to import the load_metric function from the datasets library and pass the name of the metric we want as an argument. Each metric takes as input the predicted and the ground truth values plus any additional metric-specific argument. Since we are dealing with a classification task, we add accuracy, precision, recall and F1 score to the training procedure. . We can check the full list of metrics available using the list_metrics function from the datasets library. . from datasets import load_metric accuracy_metric = load_metric(&#39;accuracy&#39;) precision_metric = load_metric(&#39;precision&#39;) recall_metric = load_metric(&#39;recall&#39;) f1_metric = load_metric(&#39;f1&#39;) . print(accuracy_metric.compute(predictions=preds, references=predictions.label_ids)) print(precision_metric.compute(predictions=preds, references=predictions.label_ids, average=&#39;weighted&#39;)) print(recall_metric.compute(predictions=preds, references=predictions.label_ids, average=&#39;weighted&#39;)) print(f1_metric.compute(predictions=preds, references=predictions.label_ids, average=&#39;weighted&#39;)) . {&#39;accuracy&#39;: 0.929} {&#39;precision&#39;: 0.928007871781864} {&#39;recall&#39;: 0.929} {&#39;f1&#39;: 0.9283332983256687} . from datasets import list_metrics list_metrics() . The Trainer class has an ad hoc parameter called compute_metrics that enables us to pass a function to compute the metrics during the evaluation phase. This function takes a PredictionOutput object as input and returns a dictionary containing the metric values. . Let&#39;s define this function and manually test it on the predictions we got a few moments ago. . def compute_metrics_fn(eval_preds): metrics = dict() accuracy_metric = load_metric(&#39;accuracy&#39;) precision_metric = load_metric(&#39;precision&#39;) recall_metric = load_metric(&#39;recall&#39;) f1_metric = load_metric(&#39;f1&#39;) logits = eval_preds.predictions labels = eval_preds.label_ids preds = np.argmax(logits, axis=-1) metrics.update(accuracy_metric.compute(predictions=preds, references=labels)) metrics.update(precision_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) metrics.update(recall_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) metrics.update(f1_metric.compute(predictions=preds, references=labels, average=&#39;weighted&#39;)) return metrics . metrics = compute_metrics_fn(predictions) metrics . {&#39;accuracy&#39;: 0.929, &#39;f1&#39;: 0.9283332983256687, &#39;precision&#39;: 0.928007871781864, &#39;recall&#39;: 0.929} . At a first glance, it seems our compute_metrics_fn works as expected. Let&#39;s run again the training loop with just a few changes: . train for 3 epochs, rather than just a single one. . | pass larger batches to the model by setting the per_device_train_batch_size argument. . | keep the best model by setting load_best_model_at_end=True. . | compute the metrics by passing the function we have just created to the training loop through the compute_metrics argument. . | . model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) # define training arguments training_args = TrainingArguments( &#39;trainer-run#0002&#39;, evaluation_strategy=&#39;epoch&#39;, save_strategy=&#39;epoch&#39;, num_train_epochs=3, per_device_train_batch_size=16, fp16=True ) . trainer = Trainer( model, training_args, train_dataset=tokenized_datasets[&#39;train&#39;], eval_dataset=tokenized_datasets[&#39;validation&#39;], data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics_fn, ) . Using amp half precision backend . trainer.train() . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 16000 Num Epochs = 3 Instantaneous batch size per device = 16 Total train batch size (w. parallel, distributed &amp; accumulation) = 16 Gradient Accumulation steps = 1 Total optimization steps = 3000 . . [3000/3000 07:26, Epoch 3/3] Epoch Training Loss Validation Loss Accuracy Precision Recall F1 . 1 | 0.226400 | 0.199119 | 0.935000 | 0.940978 | 0.935000 | 0.935969 | . 2 | 0.119400 | 0.132668 | 0.935000 | 0.935721 | 0.935000 | 0.934792 | . 3 | 0.078700 | 0.173672 | 0.938500 | 0.940526 | 0.938500 | 0.938868 | . &lt;/div&gt; &lt;/div&gt; ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 Saving model checkpoint to trainer-run#0002/checkpoint-1000 Configuration saved in trainer-run#0002/checkpoint-1000/config.json Model weights saved in trainer-run#0002/checkpoint-1000/pytorch_model.bin tokenizer config file saved in trainer-run#0002/checkpoint-1000/tokenizer_config.json Special tokens file saved in trainer-run#0002/checkpoint-1000/special_tokens_map.json ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 Saving model checkpoint to trainer-run#0002/checkpoint-2000 Configuration saved in trainer-run#0002/checkpoint-2000/config.json Model weights saved in trainer-run#0002/checkpoint-2000/pytorch_model.bin tokenizer config file saved in trainer-run#0002/checkpoint-2000/tokenizer_config.json Special tokens file saved in trainer-run#0002/checkpoint-2000/special_tokens_map.json ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 Saving model checkpoint to trainer-run#0002/checkpoint-3000 Configuration saved in trainer-run#0002/checkpoint-3000/config.json Model weights saved in trainer-run#0002/checkpoint-3000/pytorch_model.bin tokenizer config file saved in trainer-run#0002/checkpoint-3000/tokenizer_config.json Special tokens file saved in trainer-run#0002/checkpoint-3000/special_tokens_map.json Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=3000, training_loss=0.2011304931640625, metrics={&#39;train_runtime&#39;: 446.3705, &#39;train_samples_per_second&#39;: 107.534, &#39;train_steps_per_second&#39;: 6.721, &#39;total_flos&#39;: 1161520801033536.0, &#39;train_loss&#39;: 0.2011304931640625, &#39;epoch&#39;: 3.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; trainer.evaluate(tokenized_datasets[&#39;test&#39;]) . ***** Running Evaluation ***** Num examples = 2000 Batch size = 8 . . [250/250 00:06] {&#39;epoch&#39;: 3.0, &#39;eval_accuracy&#39;: 0.93, &#39;eval_f1&#39;: 0.9304119781172865, &#39;eval_loss&#39;: 0.18988770246505737, &#39;eval_precision&#39;: 0.9323717575998934, &#39;eval_recall&#39;: 0.93, &#39;eval_runtime&#39;: 7.6352, &#39;eval_samples_per_second&#39;: 261.943, &#39;eval_steps_per_second&#39;: 32.743} . WOW!ü•Ç After training for just 3 epochs we reach almost 95% in accuracy on the test set and the other metrics look good. . &#129680; Adding the classification report and confusion matrix &#129680; . Let‚Äôs define a function to automatically convert logits into label IDs. Then we import the classification_report and ConfusionMatrixDisplay functions from sklearn.metrics and pass the ground truth and predicted labels to those two functions. The classification_report function generates a detailed report providing additional information for each label, whereas the ConfusionMatrixDisplay function returns a neat confusion matrix showing where our sentence-classification model performs well and where it falls short. For instance, by looking at the numbers in the confusion matrix, it is apparent that the model sometimes mixes up joy and love and misinterprets fear as surprise. . def get_predictions(dataset, model): raw_preds = model.predict(dataset) logits = raw_preds.predictions targets = raw_preds.label_ids preds = np.argmax(logits, axis=-1) return (targets, preds) # get indexes and target labels targets, preds = get_predictions(tokenized_datasets[&#39;test&#39;], trainer) . ***** Running Prediction ***** Num examples = 2000 Batch size = 8 . . [250/250 04:22] from sklearn.metrics import classification_report print(classification_report(targets, preds, target_names=label_names)) . precision recall f1-score support sadness 0.96 0.97 0.97 581 joy 0.97 0.93 0.95 695 love 0.79 0.92 0.85 159 anger 0.93 0.91 0.92 275 fear 0.87 0.91 0.89 224 surprise 0.79 0.70 0.74 66 accuracy 0.93 2000 macro avg 0.89 0.89 0.89 2000 weighted avg 0.93 0.93 0.93 2000 . from sklearn.metrics import ConfusionMatrixDisplay from matplotlib import pyplot as plt fig, ax = plt.subplots(dpi=100) ConfusionMatrixDisplay.from_predictions(targets, preds, display_labels=label_names, cmap=&#39;GnBu&#39;, ax=ax ) plt.show() . &#129680; Saving the model checkpoint &#129680; . This few bits of code show how to save a model checkpoint locally or on the cloud: . we zip the checkpoint folder we want to save using shutil.make_archive. The first argument is the name of the file to create, the second is the format and the third is the input directory. . | we use shutil.copy to copy the zip folder to another location. In this example, we copy the checkpoint on Google Drive. . | . ‚ÄºÔ∏è Make sure to adjust the base_name, root_dir, src and dst parameters accordingly to the environment used. ‚ÄºÔ∏è . import shutil shutil.make_archive(base_name=&#39;/content/last_checkpoint&#39;, format=&#39;zip&#39;, root_dir=&#39;/content/trainer-run#0002/checkpoint-2000&#39;) . &#39;/content/last_checkpoint.zip&#39; . shutil.copy(src=&#39;/content/last_checkpoint.zip&#39;, dst=&#39;/content/drive/MyDrive/transformers/apr24_last_checkpoint.zip&#39;) . This part concludes the blog post. . Thanks for your time and attention! üôè . . &#128218; Additional resources . The Weights &amp; Biases study group is an amazing resource to get started with Hugging Face. | The Hugging Face course teaches additional topics we didn&#39;t cover in this notebook. | If you prefer books, Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf is the perfect resource to learn more about NLP and transformers. | The Illustrated Transformer and The Illustrated BERT by Jay Alammar are two great blog posts full of amazing visualizations to further explore transformers. | The Annotated Transformer by Harvard NLP goes through the original paper &quot;Attention is All You Need&quot; showing how to implement the transformer architecture in PyTorch line by line. | . &#128218; Acknowledgements . A big shout out to Sanyam Bhutani and Wayde Gilliam. The passion and enthusiasm they put in hosting the study group is what motivated me to write this blog post in the first place. Thanks to Weights &amp; Biases for organizating it. Big thanks to the Hugging Face community for putting together the Hugging Face Course. The ideas presented in this post are heavily inspired by it. . &lt;/div&gt; .",
            "url": "https://matteopilotto.github.io/blog/2022/04/24/hg_transformer_finetune_part1.html",
            "relUrl": "/2022/04/24/hg_transformer_finetune_part1.html",
            "date": " ‚Ä¢ Apr 24, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I‚Äôm Matteo and this my personal space where I share my ideas about Machine Learning topics. .",
          "url": "https://matteopilotto.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://matteopilotto.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}